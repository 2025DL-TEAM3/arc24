{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune an LLM to learn to count objects in a grid, or to solve ARC tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I might do 2 steps of fine-tuning:\n",
    "\n",
    "1. Learning priors, f.e. learning to count\n",
    "2. Solve ARC tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/ironbar/prompt_recovery/blob/main/notebooks/012_fine-tune_llama.ipynb\n",
    "- https://github.com/ironbar/prompt_recovery/blob/main/notebooks/020_fine-tune_final_ensemble.ipynb\n",
    "- https://www.kaggle.com/code/ironbar/few-shot-prompting-for-arc24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors\n",
    "import wandb\n",
    "from typing import Optional\n",
    "from itertools import product, islice, permutations, chain\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_path = \"/home/gbarbadillo/data/Phi-3-mini-128k-instruct\"\n",
    "    adapter_path: Optional[str] = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_random_question_lr1e-4_1e5dataset/checkpoint-6228'\n",
    "    train_dataset = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json'\n",
    "    val_dataset = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'\n",
    "    output_dir = '/mnt/hdd0/Kaggle/arc24/models/20240729_arc_fine_tuning/05_phi-3_start_from_count_model'\n",
    "    max_seq_len = 4096\n",
    "    epochs = 0.2\n",
    "    eval_steps = 50\n",
    "    warmup_ratio = 0.1\n",
    "    learning_rate = 1e-4\n",
    "    # LoRA\n",
    "    use_rslora = True,\n",
    "    use_dora = True,\n",
    "    lora_r = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "with open(os.path.join(cfg.output_dir, 'cfg.json'), 'w') as f:\n",
    "    json.dump({key:value for key, value in cfg.__dict__.items() if not key.startswith('__')}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    device_map = {\n",
    "        'model.embed_tokens': 0,\n",
    "        'model.layers.0': 0,\n",
    "        'model.layers.1': 0,\n",
    "        'model.layers.2': 0,\n",
    "        'model.layers.3': 0,\n",
    "        'model.layers.4': 0,\n",
    "        'model.layers.5': 0,\n",
    "        'model.layers.6': 0,\n",
    "        'model.layers.7': 0,\n",
    "        'model.layers.8': 0,\n",
    "        'model.layers.9': 0,\n",
    "        'model.layers.10': 0,\n",
    "        'model.layers.11': 0,\n",
    "        'model.layers.12': 0,\n",
    "        'model.layers.13': 0,\n",
    "        'model.layers.14': 0,\n",
    "        'model.layers.15': 0,\n",
    "        'model.layers.16': 0,\n",
    "        'model.layers.17': 1,\n",
    "        'model.layers.18': 1,\n",
    "        'model.layers.19': 1,\n",
    "        'model.layers.20': 1,\n",
    "        'model.layers.21': 1,\n",
    "        'model.layers.22': 1,\n",
    "        'model.layers.23': 1,\n",
    "        'model.layers.24': 1,\n",
    "        'model.layers.25': 1,\n",
    "        'model.layers.26': 1,\n",
    "        'model.layers.27': 1,\n",
    "        'model.layers.28': 1,\n",
    "        'model.layers.29': 1,\n",
    "        'model.layers.30': 1,\n",
    "        'model.layers.31': 1,\n",
    "        'model.norm': 1,\n",
    "        'model.rotary_emb': 1,\n",
    "        'lm_head': 1,\n",
    "    }\n",
    "else:\n",
    "    device_map = 'balanced'\n",
    "\n",
    "# device_map = 'balanced'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    # max_memory={0: '9GB', 1: '8GB'},\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_path,\n",
    "    trust_remote_code=True)\n",
    "if 'llama' in cfg.model_path:\n",
    "    print('Adding <|pad|> token to tokenizer')\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.padding_side = 'right'\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory():\n",
    "    for device in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {device} memory allocated: {torch.cuda.memory_allocated(device)/1024**3:.1f} GB, max memory allocated: {torch.cuda.max_memory_allocated(device)/1024**3:.1f} GB')\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEncoder(ABC):\n",
    "    @abstractmethod\n",
    "    def to_text(self, grid):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def to_grid(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid = np.eye(3, dtype=int).tolist()\n",
    "\n",
    "def test_translator(translator):\n",
    "    assert sample_grid == translator.to_grid(translator.to_text(sample_grid))\n",
    "    print(translator.to_text(sample_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimalGridEncoder(GridEncoder):\n",
    "    @staticmethod\n",
    "    def to_text(grid):\n",
    "        text = '\\n'.join([''.join([str(x) for x in line]) for line in grid])\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_grid(text):\n",
    "        lines = text.strip().splitlines()\n",
    "        grid = [[int(x) for x in line] for line in lines]\n",
    "        return grid\n",
    "        \n",
    "test_translator(MinimalGridEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWithSeparationEncoder(GridEncoder):\n",
    "    def __init__(self, split_symbol):\n",
    "        self.split_symbol = split_symbol\n",
    "\n",
    "    def to_text(self, grid):\n",
    "        text = '\\n'.join([self.split_symbol.join([str(x) for x in line]) for line in grid])\n",
    "        return text\n",
    "\n",
    "    def to_grid(self, text):\n",
    "        lines = text.strip().splitlines()\n",
    "        grid = [[int(x) for x in line.split(self.split_symbol)] for line in lines]\n",
    "        return grid\n",
    "\n",
    "test_translator(GridWithSeparationEncoder('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridCodeBlockEncoder(GridEncoder):\n",
    "    def __init__(self, base_encoder):\n",
    "        self.encoder = base_encoder\n",
    "\n",
    "    def to_text(self, grid):\n",
    "        text = f'```grid\\n{self.encoder.to_text(grid)}\\n```'\n",
    "        return text\n",
    "\n",
    "    def to_grid(self, text):\n",
    "        grid_text = text.split('```grid\\n')[1].split('\\n```')[0]\n",
    "        grid = self.encoder.to_grid(grid_text)\n",
    "        return grid\n",
    "\n",
    "test_translator(GridCodeBlockEncoder(MinimalGridEncoder()))\n",
    "\n",
    "test_translator(GridCodeBlockEncoder(GridWithSeparationEncoder('|')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(grid):\n",
    "    grid = np.array(grid)\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    plt.imshow(grid, cmap=cmap, norm=norm)\n",
    "    plt.grid(True,which='both',color='lightgrey', linewidth=0.5)\n",
    "    plt.xticks(np.arange(-0.5, grid.shape[1]), [])\n",
    "    plt.yticks(np.arange(-0.5, grid.shape[0]), [])\n",
    "    plt.xlim(-0.5, grid.shape[1]-0.5)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            plt.text(j, i, grid[i, j], ha='center', va='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_task(task):\n",
    "    all_samples = task['train'] + task['test']\n",
    "    for plot_idx, sample in enumerate(all_samples):\n",
    "        plt.subplot(1, len(all_samples), plot_idx+1)\n",
    "        plot_grid(sample['input'])\n",
    "        if plot_idx < len(task['train']):\n",
    "            plt.title(f'train {plot_idx}')\n",
    "        else:\n",
    "            plt.title(f'test {plot_idx-len(task[\"train\"])}')\n",
    "    plt.suptitle('Inputs for task')\n",
    "    plt.show()\n",
    "    for plot_idx, sample in enumerate(all_samples):\n",
    "        plt.subplot(1, len(all_samples), plot_idx+1)\n",
    "        plot_grid(sample['output'])\n",
    "        if plot_idx < len(task['train']):\n",
    "            plt.title(f'train {plot_idx}')\n",
    "        else:\n",
    "            plt.title(f'test {plot_idx-len(task[\"train\"])}')\n",
    "    plt.suptitle('Outputs for task')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to augment the available arc tasks:\n",
    "\n",
    "- Rotations and flips\n",
    "- Change the order of the train samples\n",
    "- Swap one of the train samples with one of the test samples\n",
    "- Remap the colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAugmentation():\n",
    "    def __init__(self, flip, n_rot90):\n",
    "        self.flip = flip\n",
    "        self.n_rot90 = n_rot90\n",
    "\n",
    "    def augment_task(self, task):\n",
    "        augmented_task = dict()\n",
    "        for partition, samples in task.items():\n",
    "            augmented_task[partition] = [{name:self.augment_grid(grid) for name,grid in sample.items()} for sample in samples]\n",
    "        return augmented_task\n",
    "\n",
    "    def augment_grid(self, grid):\n",
    "        grid = np.array(grid)\n",
    "        if self.flip:\n",
    "            grid = np.flip(grid, axis=1)\n",
    "        grid = np.rot90(grid, k=self.n_rot90)\n",
    "        return grid.tolist()\n",
    "\n",
    "    def revert_augmentation(self, grid):\n",
    "        grid = np.array(grid)\n",
    "        grid = np.rot90(grid, k=-self.n_rot90)\n",
    "        if self.flip:\n",
    "            grid = np.flip(grid, axis=1)\n",
    "        return grid.tolist()\n",
    "\n",
    "\n",
    "for flip in [True, False]:\n",
    "    for n_rot90 in range(4):\n",
    "        data_augmentation = DataAugmentation(flip, n_rot90)\n",
    "        assert sample_grid == data_augmentation.revert_augmentation(data_augmentation.augment_grid(sample_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_one_train_and_test_sample(task):\n",
    "    augmented_tasks = [task]\n",
    "    for train_idx, train_sample in enumerate(task['train']):\n",
    "        for test_idx, test_sample in enumerate(task['test']):\n",
    "            augmented_task = dict()\n",
    "            augmented_task['train'] = task['train'][:train_idx] + [test_sample] + task['train'][train_idx+1:]\n",
    "            augmented_task['test'] = task['test'][:test_idx] + [train_sample] + task['test'][test_idx+1:]\n",
    "            augmented_tasks.append(augmented_task)\n",
    "    return augmented_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_train_samples(task, max_permutations=6):\n",
    "    augmented_tasks = []\n",
    "    train_order_permutations = np.array(list(permutations(range(len(task['train'])))))\n",
    "    if len(train_order_permutations) > max_permutations:\n",
    "        indices = np.random.choice(np.arange(len(train_order_permutations)), max_permutations, replace=False)\n",
    "        train_order_permutations = train_order_permutations[indices.tolist()]\n",
    "    for train_order in train_order_permutations:\n",
    "        augmented_task = dict()\n",
    "        augmented_task['train'] = [task['train'][idx] for idx in train_order]\n",
    "        augmented_task['test'] = task['test']\n",
    "        augmented_tasks.append(augmented_task)\n",
    "    return augmented_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_geometric_augmentations(task):\n",
    "    augmented_tasks = []\n",
    "    for flip in [True, False]:\n",
    "        for n_rot90 in range(4):\n",
    "            data_augmentation = DataAugmentation(flip, n_rot90)\n",
    "            augmented_task = data_augmentation.augment_task(task)\n",
    "            augmented_tasks.append(augmented_task)\n",
    "    return augmented_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_data_augmentations(tasks):\n",
    "    print('Applying all data augmentations, initial number of tasks is', len(tasks))\n",
    "    augmented_tasks = tasks\n",
    "    augmented_tasks = list(chain(*[permute_train_samples(task) for task in tqdm(augmented_tasks)]))\n",
    "    print(f'After premuting train samples there are {len(augmented_tasks)} tasks')\n",
    "    augmented_tasks = list(chain(*[apply_all_geometric_augmentations(task) for task in tqdm(augmented_tasks)]))\n",
    "    print(f'After applying geometric augmentations there are {len(augmented_tasks)} tasks')\n",
    "    augmented_tasks = list(chain(*[swap_one_train_and_test_sample(task) for task in tqdm(augmented_tasks)]))\n",
    "    print(f'After swapping train and test samples there are {len(augmented_tasks)} tasks')\n",
    "    return augmented_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"\"\"You are a helpful AI assistant. Your job is to solve tasks from the Abstraction and Reasoning Challenge (ARC). \n",
    "The user will present you with sample input and output grids for each task. \n",
    "Your job will be to understand the transformation between the input and the output and apply it to the last input grid given by the user. \n",
    "The puzzle-like inputs and outputs present a grid where each square can be one of ten colors. A grid can be any height or width between 1x1 and 30x30.\n",
    "The background of the grid is typically colored with 0.\n",
    "The tasks from ARC are based on the following priors:\n",
    "\n",
    "- Objectness: Objects persist and cannot appear or disappear without reason. Objects can interact or not depending on the circumstances.\n",
    "- Goal-directed: Objects can be animate or inanimate. Some objects are \"agents\" - they have intentions and they pursue goals.\n",
    "- Numbers & counting: Objects can be counted or sorted by their shape, appearance, or movement using basic mathematics like addition, subtraction, and comparison.\n",
    "- Basic geometry & topology: Objects can be shapes like rectangles, triangles, and circles which can be mirrored, rotated, translated, deformed, combined, repeated, etc. Differences in distances can be detected.\n",
    "\n",
    "The transformations between input and output should be based on these priors.\n",
    "\"\"\"\n",
    "\n",
    "def create_prompts_from_task(task, grid_encoder):\n",
    "    prompts = []\n",
    "    for test_sample in task['test']:\n",
    "        messages = [{\"role\": \"system\", \"content\": task_description}]\n",
    "        user_message = \"Let's see if you can solve this simple ARC task. These are some input-output grid examples that define the task.\\n\"\n",
    "        for example_idx, sample in enumerate(task['train']):\n",
    "            user_message += f\"\\n## Example {example_idx}\\n\\n### Input\\n\\n{grid_encoder.to_text(sample['input'])}\\n\"\n",
    "            user_message += f\"### Output\\n\\n{grid_encoder.to_text(sample['output'])}\\n\"\n",
    "        user_message += f\"\\n## Test case\\n\\n### Input\\n\\n{grid_encoder.to_text(test_sample['input'])}\\n\"\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"### Output\\n\\n{grid_encoder.to_text(test_sample['output'])}\\n\"})\n",
    "\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=False)\n",
    "        prompts.append(prompt)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filepath, grid_encoder, use_data_augmentation=True):\n",
    "    data = load_arc_data_with_solutions(filepath)\n",
    "\n",
    "    tasks = list(data.values())\n",
    "    if use_data_augmentation:\n",
    "        tasks = apply_all_data_augmentations(tasks)\n",
    "\n",
    "    prompts = []\n",
    "    for task in tqdm(tasks):\n",
    "        prompts.extend(create_prompts_from_task(task, grid_encoder))\n",
    "    print(len(prompts))\n",
    "\n",
    "    np.random.shuffle(prompts)\n",
    "    pretty_print_prompt(prompts[0])\n",
    "\n",
    "    prompt_lengths = [len(tokenizer.encode(prompt)) for prompt in tqdm(prompts)]\n",
    "    plt.hist(prompt_lengths, bins=100);\n",
    "    plt.title('Prompt length distribution')\n",
    "    plt.xlabel('Number of tokens');\n",
    "    plt.show()\n",
    "\n",
    "    prompts = [prompt for prompt, prompt_length in zip(prompts, prompt_lengths) if prompt_length < cfg.max_seq_len]\n",
    "    print(f'Leaving {len(prompts)} prompts after removing those longer than {cfg.max_seq_len} tokens')\n",
    "\n",
    "    dataset = Dataset.from_dict({'text': prompts})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_arc_data_with_solutions(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    with open(filepath.replace('challenges.json', 'solutions.json'), 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    for sample_id, task in data.items():\n",
    "        for idx, sample in enumerate(task['test']):\n",
    "            sample['output'] = solutions[sample_id][idx]\n",
    "    return data\n",
    "\n",
    "\n",
    "def pretty_print_prompt(text, default_color='white'):\n",
    "    color = default_color\n",
    "    attrs = None\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith('<|assistant|>'):\n",
    "            color = 'blue'\n",
    "        elif line.startswith('<|user|>'):\n",
    "            color = default_color\n",
    "        elif line.startswith('<|system|>'):\n",
    "            color = 'green'\n",
    "        if line.startswith('<'):\n",
    "            attrs = ['bold']\n",
    "        else:\n",
    "            attrs = None\n",
    "        print(colored(line, color, attrs=attrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    # we need to add separation between numbers in the grid\n",
    "    grid_encoder = GridCodeBlockEncoder(GridWithSeparationEncoder('|'))\n",
    "else:\n",
    "    grid_encoder = GridCodeBlockEncoder(MinimalGridEncoder())\n",
    "train_dataset = create_dataset(cfg.train_dataset, grid_encoder, use_data_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = create_dataset(cfg.val_dataset, grid_encoder, use_data_augmentation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.adapter_path is None:\n",
    "    peft_config = LoraConfig(\n",
    "        # lora_alpha: LoRA scaling factor.\n",
    "        lora_alpha=64, #64,\n",
    "        lora_dropout=0.1, # 0.1, althought Vaca suggested to use 0.05 for big models\n",
    "        # r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "        r=cfg.lora_r, #16\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        # target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "        target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj'],\n",
    "        use_rslora=cfg.use_rslora,\n",
    "        use_dora=cfg.use_dora,\n",
    "    )\n",
    "else:\n",
    "    print(f'Loading adapter from {cfg.adapter_path}')\n",
    "    peft_config = None\n",
    "    model = PeftModel.from_pretrained(model, cfg.adapter_path, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    batch_size_kwargs = dict(\n",
    "        per_device_train_batch_size=3, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=5,\n",
    "        per_device_eval_batch_size=4,\n",
    "    )\n",
    "else:\n",
    "    batch_size_kwargs = dict(\n",
    "        per_device_train_batch_size=1, # 4-16 should be fine for lora.\n",
    "        gradient_accumulation_steps=16,\n",
    "        per_device_eval_batch_size=2,\n",
    "    )\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=cfg.output_dir,\n",
    "        num_train_epochs=cfg.epochs,\n",
    "        warmup_ratio=cfg.warmup_ratio,\n",
    "        learning_rate=cfg.learning_rate,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "\n",
    "        do_eval=True,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_steps=cfg.eval_steps,\n",
    "        logging_steps=10, #50,\n",
    "        eval_steps=cfg.eval_steps,\n",
    "        log_level=\"debug\",\n",
    "\n",
    "        **batch_size_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'llama' in cfg.model_path:\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_template='<|start_header_id|>user<|end_header_id|>',\n",
    "        response_template='<|start_header_id|>assistant<|end_header_id|>',\n",
    "    )\n",
    "else:\n",
    "    data_collator = DataCollatorForCompletionOnlyLM(\n",
    "        tokenizer=tokenizer,\n",
    "        instruction_template='<|user|>',\n",
    "        response_template='<|assistant|>'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = wandb.init(reinit=True,\n",
    "               dir=cfg.output_dir,\n",
    "               project=os.path.basename(os.path.dirname(cfg.output_dir)),\n",
    "               name=os.path.basename(cfg.output_dir))\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=cfg.max_seq_len,\n",
    "    data_collator=data_collator,\n",
    "    args=training_arguments,\n",
    "    # packing=True, # ValueError: You passed a `DataCollatorForCompletionOnlyLM` to the SFTTrainer. This is not compatible with the `packing` argument.\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "w.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data augmentation clearly helps.\n",
    "- Training from a model that has learned to count has a bigger loss at the beginning, remember that evaluations with those models give very bad predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/docs/transformers/en/peft\n",
    "- https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.val_dataset, 'r') as f:\n",
    "    data = json.load(f)\n",
    "val_samples_ids = list(data.keys())\n",
    "\n",
    "def ask_question_to_model(sample_idx, pipe, question_idx=0, arbitrary_question=None):\n",
    "    sample_id = val_samples_ids[sample_idx]\n",
    "    sample = data[sample_id]\n",
    "\n",
    "    sample_with_one_question = sample.copy()\n",
    "    if arbitrary_question is None:\n",
    "        sample_with_one_question['questions'] = {question:answer for idx, (question, answer) in enumerate(sample['questions'].items()) if idx == question_idx}\n",
    "    else:\n",
    "        sample_with_one_question['questions'] = {arbitrary_question:''}\n",
    "\n",
    "    messages = create_messages_from_sample(sample_with_one_question, grid_encoder)\n",
    "    prompt = tokenizer.apply_chat_template(messages[:2],\n",
    "                                            tokenize=False,\n",
    "                                            add_generation_prompt=True)\n",
    "    plot_grid(sample['grid']); plt.show()\n",
    "    # pretty_print_prompt(prompt)\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"return_full_text\": False,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(prompt, **generation_args)\n",
    "    print(list(sample_with_one_question['questions'].keys())[0])\n",
    "    print(f\">{output[0]['generated_text']} ({list(sample_with_one_question['questions'].values())[0]})\")\n",
    "\n",
    "def plot_grid(grid):\n",
    "    grid = np.array(grid)\n",
    "    cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "    norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    plt.imshow(grid, cmap=cmap, norm=norm)\n",
    "    plt.grid(True,which='both',color='lightgrey', linewidth=0.5)\n",
    "    plt.xticks(np.arange(-0.5, grid.shape[1]), [])\n",
    "    plt.yticks(np.arange(-0.5, grid.shape[0]), [])\n",
    "    plt.xlim(-0.5, grid.shape[1]-0.5)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            plt.text(j, i, grid[i, j], ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/11_lr_4e-4_1e5dataset_r32/checkpoint-12400'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/09_lr_1e-3_1e4dataset_r32/checkpoint-600/'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/14_llama31/checkpoint-333'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/15_continue_training_phi3_4e5/checkpoint-22800'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_random_question_lr1e-4_1e5dataset/checkpoint-6228'\n",
    "adapter_path = '/mnt/hdd0/Kaggle/arc24/models/20240724_first_trainings/22_llama31_lr1e-4_1e5dataset_r32/checkpoint-6553'\n",
    "model.load_adapter(adapter_path, adapter_path)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to visualize the grid and ask some random question, see how well it does.\n",
    "\n",
    "Compare the responses with and without the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_question_to_model(sample_idx=750, question_idx=1, pipe=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask_question_to_model(sample_idx=300, arbitrary_question='Please describe the grid, saying how many objects are there, their color and area.', pipe=pipe)\n",
    "# ask_question_to_model(sample_idx=300, arbitrary_question='What is the shape of the grid? (nxn)', pipe=pipe)\n",
    "ask_question_to_model(sample_idx=550, arbitrary_question='Describe the objects in the grid', pipe=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Measure the effect of using data augmentation\n",
    "- [ ] Color swapping data augmentation, map the colors into a random new space.\n",
    "- [ ] IterableDataset. https://huggingface.co/docs/datasets/en/about_mapstyle_vs_iterable#creating-map-style-datasets-and-iterable-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prometeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
