{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take the inference of a model and evaluate and visualize it.\n",
    "\n",
    "This will help to:\n",
    "\n",
    "- understand the failures of the model\n",
    "- find a better way to combine the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/evaluations/20240914_overfit_to_train/02_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_1e5steps/checkpoint-100000/inference_training_x032.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15_x128_voting.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_t01.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_t08.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15.json'\n",
    "    solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/debug/first_predictions/checkpoint-14000/inference_evaluation_x032.json'\n",
    "    solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/evaluations/20241014_omni-arc_improvements/02_omni-arc-269-Qwen2.5-0.5B-Instruct_lora064_lr1e-4_bs32_20000steps_7168msl/checkpoint-20000/inference_evaluation_x008_t7e-01.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/debug/third_model/checkpoint-26000/inference_evaluation_x008_t7e-1.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# add path to python path\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions, evaluate,\n",
    "    study_effect_of_the_number_of_solutions,\n",
    "    study_attempt_accuracy,\n",
    "    print_metrics,\n",
    "    visualize_tasks_and_predictions)\n",
    "from voting import (\n",
    "    select_most_voted_solutions,\n",
    "    select_most_voted_solutions_solving_ties_with_logprob\n",
    ")\n",
    "from arc24.prompting import pretty_print_prompt\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 3)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_checkpoints(parent_folder):\n",
    "    ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "    all_metrics = dict()\n",
    "    for folder in sorted(glob.glob(os.path.join(parent_folder, '*'))):\n",
    "        print(folder)\n",
    "        filepaths = sorted(glob.glob(os.path.join(folder, '*/inference*.json')), key=lambda x: int(x.split('checkpoint-')[-1].split('/inference')[0]))\n",
    "        filepaths = [filepath for filepath in sorted(filepaths) if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')]\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath, 'r') as f:\n",
    "                solutions = json.load(f)\n",
    "            print(filepath)\n",
    "            metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "            metrics['n'] = len(list(solutions.values())[0][0])\n",
    "            with open(filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "                task_results = json.load(f)\n",
    "            for i in range(1, 3):\n",
    "                metrics[f'vote_{i}'] = evaluate(ground_truth, select_most_voted_solutions_solving_ties_with_logprob(task_results, i), verbose=False)[0].get('pass_n', 0)\n",
    "\n",
    "            print_metrics(metrics)\n",
    "            all_metrics[filepath] = {key: value for key, value in metrics.items() if key != 'max_correct_pixels' and key != 'any_correct_size'}\n",
    "        print()\n",
    "    df = pd.DataFrame(all_metrics).T\n",
    "    df = df[['accuracy', 'pass_n', 'vote_2', 'vote_1', 'correct_pixels', 'correct_size', 'unanswered', 'n']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temperature_from_filepath(filepath):\n",
    "    try:\n",
    "        return float(filepath.split('_t')[1].split('.json')[0])\n",
    "    except IndexError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_task_prompt(task_results, task_id, result_idx=0):\n",
    "    task_id_results = [result for result in task_results if result['task_id'] == task_id]\n",
    "    result = task_id_results[result_idx]\n",
    "    pretty_print_prompt(result['prompt'] + result['response'], default_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "data = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "evaluate(data, solutions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "study_effect_of_the_number_of_solutions(solutions, data, n_tries=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(data, select_most_voted_solutions(solutions, 2), verbose=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_attempt_accuracy(solutions, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tasks_and_predictions(solutions, data, only_correct=True, ascending=True, max_predictions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_analysis(model):\n",
    "    metrics, temperature = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_{model}_x128_t*.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        temperature.append(float(filepath.split('_t')[1].split('.json')[0])/10)\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.plot(temperature, [m[key] for m in metrics], 'o-')\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('temperature')\n",
    "    plt.suptitle(f'Effect of the temperature on the model {model}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_analysis(model='qwen05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_analysis(model='qwen15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study influence of output grid shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "data = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "global_metrics, task_metrics = evaluate(data, solutions, verbose=False)\n",
    "task_ids = list(task_metrics.keys())\n",
    "global_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_grid_shape(data):\n",
    "    shapes = dict()\n",
    "    for task_id, task in data.items():\n",
    "        output_shapes = [np.array(sample['output']).shape for sample in task['test'] + task['train']]\n",
    "        shapes[task_id] = np.mean(output_shapes, axis=0)\n",
    "    return shapes\n",
    "\n",
    "output_shapes = get_output_grid_shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(global_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'pass_n', 'unanswered'] #'correct_size', \n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(global_metrics), plot_idx + 1)\n",
    "    x = [output_shapes[task_id][1] for task_id in task_ids]\n",
    "    y = [output_shapes[task_id][0] for task_id in task_ids]\n",
    "    c = [task_metrics[task_id][key] for task_id in task_ids]\n",
    "    plt.scatter(x, y, c=c, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title(key)\n",
    "    plt.xlabel('cols')\n",
    "    plt.ylabel('rows')\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the output shape on the model performance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo beam-search analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "\n",
    "def pseudo_beam_search_analysis_v1():\n",
    "    metrics, n = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x8_T01_n*.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        n.append(int(filepath.split('_n')[1].split('.json')[0]))\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.scatter(n, [m[key] for m in metrics], )\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('n')\n",
    "    # plt.suptitle(f'Effect of the temperature on the model {model}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "pseudo_beam_search_analysis_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear result, let's do another analysis with different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "def pseudo_beam_search_analysis_v2(x):\n",
    "    metrics, temperature = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x{x}_n20_T??.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        print(filepath)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        temperature.append(float(filepath.split('_T')[1].split('.json')[0])/10)\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.plot(temperature, [m[key] for m in metrics], 'o-')\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('temperature')\n",
    "    plt.suptitle(f'Effect of the temperature on the model for {x} predictions')\n",
    "    plt.tight_layout()\n",
    "\n",
    "pseudo_beam_search_analysis_v2(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_beam_search_analysis_v2(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_n20_T08.json', 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "print(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15_x128.json', 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "print(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation loss vs metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "filepaths = sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/*/*/inference*.json'))\n",
    "filepaths = [filepath for filepath in filepaths if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')]\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    print(filepath)\n",
    "    metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "    print_metrics(metrics)\n",
    "    # metrics = evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]\n",
    "    # print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "filepaths = sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/06*/*/inference*.json'))\n",
    "filepaths = [filepath for filepath in filepaths if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')][:-1]\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    print(filepath)\n",
    "    metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "    print_metrics(metrics)\n",
    "    # metrics = evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]\n",
    "    # print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"checkpoint_steps\": [500, 1000, 2000, 2650, 3000, 3450, 4000, 4850, 6000],\n",
    "    \"accuracy\": [0.4, 0.6, 1.1, 1.1, 1.3, 1.4, 2.2, 2.3, 2.3],\n",
    "    \"correct_pixels\": [58.3, 62.1, 63.8, 64.2, 65.0, 65.3, 65.4, 65.8, 66.7],\n",
    "    \"max_correct_pixels\": [76.7, 76.5, 80.6, 80.6, 80.5, 82.4, 83.0, 81.6, 81.7],\n",
    "    \"correct_size\": [78.0, 81.1, 83.4, 82.4, 84.0, 84.2, 84.3, 84.2, 84.9],\n",
    "    \"any_correct_size\": [88.5, 89.0, 90.0, 90.5, 90.0, 92.0, 92.0, 90.0, 89.5],\n",
    "    \"pass_n\": [6.5, 5.5, 10.0, 11.5, 10.5, 11.0, 15.0, 15.5, 15.0],\n",
    "    \"unanswered\": [7.1, 5.1, 5.6, 5.7, 4.3, 4.3, 4.4, 4.8, 4.1],\n",
    "    'val_loss': [0.237, 0.198, 0.169, 0.171, 0.162, 0.162, 0.159, 0.148, 0.159]\n",
    "}\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'correct_size', 'unanswered']\n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(keys), plot_idx + 1)\n",
    "    plt.plot(metrics['checkpoint_steps'], metrics[key], 'o-', label=key)\n",
    "    plt.grid()\n",
    "    plt.xlabel('checkpoint_steps')\n",
    "    plt.ylabel(key)\n",
    "plt.suptitle('Effect of the number of checkpoint steps on the model performance')\n",
    "plt.show()\n",
    "\n",
    "keys = ['pass_n', 'any_correct_size', 'max_correct_pixels', 'val_loss']\n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(keys), plot_idx + 1)\n",
    "    plt.plot(metrics['checkpoint_steps'], metrics[key], 'o-', label=key)\n",
    "    plt.grid()\n",
    "    plt.xlabel('checkpoint_steps')\n",
    "    plt.ylabel(key)\n",
    "plt.suptitle('Effect of the number of checkpoint steps on the model performance')\n",
    "plt.show()\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'correct_size', 'unanswered']\n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(keys), plot_idx + 1)\n",
    "    plt.plot(metrics['val_loss'], metrics[key], 'o-', label=key)\n",
    "    plt.grid()\n",
    "    plt.xlabel('val_loss')\n",
    "    plt.ylabel(key)\n",
    "plt.suptitle('Effect of the number of checkpoint steps on the model performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20240928_smolLM/01_full-fine-tuning-SmolLM-135M-Instruct_lr4e-4_1e4steps_1gpus_8192msl/checkpoint-10000/inference_evaluation_x032_task_results.json'\n",
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20240929_smolLM/01_full-fine-tuning-SmolLM-135M-Instruct_lr8e-4_1e3steps_1gpus_8192msl/checkpoint-1000/inference_evaluation_x034_task_results.json'\n",
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20241009_optimize_code_generation/01_omni-arc-400-code-from-examples-v2-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_2000steps_2gpus_8192msl/checkpoint-2000/inference_evaluation_x008_task_results.json'\n",
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20241011_non-instruct_models/02_baseline_instruct/checkpoint-100/inference_smaller_5_tasks_x008_task_results.json'\n",
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20241011_non-instruct_models/03_full-fine-tune/checkpoint-100/inference_smaller_5_tasks_x009_task_results.json'\n",
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20241011_non-instruct_models/07_final_experiment_with_lora_longer/checkpoint-500/inference_smaller_5_tasks_x008_task_results.json'\n",
    "with open(filepath, 'r') as f:\n",
    "    task_results = json.load(f)\n",
    "len(task_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = task_results[0]['task_id']\n",
    "print(f'https://arcprize.org/play?task={task_id}')\n",
    "inspect_task_prompt(task_results, task_id, result_idx=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_task_prompt(task_results, '00576224', result_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper study on the number of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_predictions(folder):\n",
    "    filepaths = glob.glob(os.path.join(folder, 'inference*.json'))\n",
    "    filepaths = [filepath for filepath in filepaths if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')]\n",
    "    with open(filepaths[0], 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    for filepath in tqdm(filepaths[1:], desc='loading predictions'):\n",
    "        with open(filepath, 'r') as f:\n",
    "            additional_predictions = json.load(f)\n",
    "        for task_id, solutions in additional_predictions.items():\n",
    "            for test_idx, solution in enumerate(solutions):\n",
    "                for prediction in solution.values():\n",
    "                    attempt_name = f'attempt_{len(predictions[task_id][test_idx]) + 1}'\n",
    "                    predictions[task_id][test_idx][attempt_name] = prediction\n",
    "    print(f'Collected {len(list(predictions.keys()))} tasks, each with {len(list(predictions.values())[0][0])} predictions')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_5e3steps_2gpus_8192msl/checkpoint-5000',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_1e4steps_2gpus_8192msl/checkpoint-10000',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_2e4steps_2gpus_8192msl/checkpoint-20000',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl/checkpoint-40000',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000',\n",
    "]\n",
    "for folder in folders:\n",
    "    print(folder)\n",
    "    predictions = collect_all_predictions(folder=folder)\n",
    "    plt.figure(figsize=(25, 5))\n",
    "    training_steps = int(float(folder.split('steps_')[0].split('_')[-1]))\n",
    "    study_effect_of_the_number_of_solutions(predictions, data, n_tries=100, title=f'{training_steps} training steps', min_predictions=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate multiple checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "#/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/04_row-number-and-grid-shape_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps/checkpoint-6000/inference.json\n",
    "#accuracy: 2.8%\tcorrect_pixels: 66.3%\tmax_correct_pixels: 82.2%\tcorrect_size: 84.2%\tany_correct_size: 91.0%\tpass_n: 18.5%\tunanswered: 2.8%\n",
    "#accuracy: 3.5%\tcorrect_pixels: 67.4%\tmax_correct_pixels: 72.2%\tcorrect_size: 85.4%\tany_correct_size: 87.9%\tpass_n: 7.1%\tunanswered: 2.5%\t\n",
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240828_grid_encoders_ttft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240901_data_scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240902_external_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240905_external_data_v2')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['group'] = [x.split('/')[-3].replace('msl_c', 'msl').replace('msl_b', 'msl') for x in df.index]\n",
    "df[df.n == 32].groupby('group').mean().style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240910_predict_inputs')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240903_submission_models')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240913_more_data/')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240914_overfit_to_train/')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240921_overfit_to_train/')\n",
    "df['training_steps'] = [int(float(x.split('steps_')[0].split('_')[-1])) for x in df.index]\n",
    "df['dataset'] = list(map(lambda x: 'training' if 'inference_training' in x else 'evaluation', df.index))\n",
    "df.style.format(\"{:.2%}\", subset=['accuracy', 'pass_n', 'vote_2', 'vote_1', 'correct_pixels', 'correct_size', 'unanswered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/')\n",
    "df['training_steps'] = [float(x.split('steps_')[0].split('_')[-1]) for x in df.index]\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240929_smolLM/')\n",
    "df['training_steps'] = [float(x.split('steps_')[0].split('_')[-1]) for x in df.index]\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20241006_omniarc_validation/')\n",
    "df['training_steps'] = [float(x.split('steps_')[0].split('_')[-1]) for x in df.index]\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20241007_batch_size/')\n",
    "df['batch_size'] = [int(x.split('_lr')[0].split('_bs')[-1]) for x in df.index]\n",
    "df['lr'] = [float(x.split('_Qwen')[0].split('_lr')[-1]) for x in df.index]\n",
    "df.sort_values(['batch_size', 'lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/debug')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20241009_optimize_code_generation/')\n",
    "df['training_steps'] = [float(x.split('steps_')[0].split('_')[-1]) for x in df.index]\n",
    "df['temperature'] = [get_temperature_from_filepath(x) for x in df.index]\n",
    "df['experiment'] = [x.split('/')[-3] for x in df.index]\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.temperature == 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment, sub_df in df[df.experiment != '04_omni-arc-150-output-from-examples-v1-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_2000steps_2gpus_8192msl'].groupby('experiment'):\n",
    "    plt.plot(sub_df['temperature'], sub_df['pass_n'], 'o-', label=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.experiment != '04_omni-arc-150-output-from-examples-v1-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_2000steps_2gpus_8192msl'][['temperature', 'pass_n', 'accuracy', 'vote_1']].groupby('temperature').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df[df.experiment != '04_omni-arc-150-output-from-examples-v1-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_2000steps_2gpus_8192msl'][['temperature', 'pass_n']].groupby('temperature').mean()\n",
    "results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results.index, results['pass_n'], 'o-')\n",
    "plt.grid()\n",
    "plt.xlabel('temperature')\n",
    "plt.ylabel('pass_n')\n",
    "plt.title('Effect of the temperature on the model performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = df[df.temperature == 0].head(5).sort_values('training_steps')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results['training_steps'], results['pass_n'], 'o-')\n",
    "plt.grid()\n",
    "plt.xlabel('training steps')\n",
    "plt.ylabel('pass_n')\n",
    "plt.title('Effect of the training steps on the model performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('experiment')[['pass_n', 'vote_2', 'vote_1']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.experiment == '03_omni-arc-333-code-from-examples-v2-Qwen2.5-0.5B-Instruct_lora128_lr1e-4_bs32_8000steps_2gpus_8192msl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.temperature == 0) & (df.experiment.apply(lambda x: x.startswith('01') or x.startswith('02')))]['pass_n'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.temperature == 0) & (df.experiment.apply(lambda x: x.startswith('03') or x.startswith('05') ))]['pass_n'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.temperature == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20241012_coding_models/')\n",
    "df['training_steps'] = [float(x.split('steps_')[0].split('_')[-1]) for x in df.index]\n",
    "df['temperature'] = [get_temperature_from_filepath(x) for x in df.index]\n",
    "# df['experiment'] = [x.split('/')[-3] for x in df.index]\n",
    "df.index = [x.split('/')[-3] for x in df.index]\n",
    "df['model'] = ['1.5B' if '1.5B' in x else '7B' for x in df.index]\n",
    "df['lora'] = [int(x.split('lora')[-1].split('_')[0]) for x in df.index]\n",
    "# df.style.format(\"{:.2%}\")\n",
    "df.sort_values(['model', 'training_steps', 'lora'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20241014_omni-arc_improvements/')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Analyze number of succesfull predictions per task, that is the unanswered metric!\n",
    "- [x] How the number of predictions affects the metrics\n",
    "- [x] Sort the tasks by accuracy, correct pixels and correct size\n",
    "- [x] Visualize the tasks, sorted by accuracy\n",
    "- [x] Visualize the effect of grid shape in the metrics\n",
    "- [x] Accuracy of each attempt\n",
    "- [x] Dynamically choose ground truth based on number of predicted tasks (There's no need)\n",
    "- [x] Show vote_2 metric always\n",
    "- [x] Also show vote_1 metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
