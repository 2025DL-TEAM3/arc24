{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take the inference of a model and evaluate and visualize it.\n",
    "\n",
    "This will help to:\n",
    "\n",
    "- understand the failures of the model\n",
    "- find a better way to combine the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_debug.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15_x128_voting.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_t01.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_t08.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# add path to python path\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions, evaluate,\n",
    "    study_effect_of_the_number_of_solutions,\n",
    "    study_attempt_accuracy,\n",
    "    print_metrics,\n",
    "    visualize_tasks_and_predictions)\n",
    "from voting import (\n",
    "    select_most_voted_solutions,\n",
    "    select_most_voted_solutions_solving_ties_with_logprob\n",
    ")\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multiple_checkpoints(parent_folder):\n",
    "    ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "    all_metrics = dict()\n",
    "    for folder in sorted(glob.glob(os.path.join(parent_folder, '*'))):\n",
    "        print(folder)\n",
    "        filepaths = sorted(glob.glob(os.path.join(folder, '*/inference*.json')), key=lambda x: int(x.split('checkpoint-')[-1].split('/inference')[0]))\n",
    "        filepaths = [filepath for filepath in sorted(filepaths) if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')]\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath, 'r') as f:\n",
    "                solutions = json.load(f)\n",
    "            print(filepath)\n",
    "            metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "            metrics['n'] = len(list(solutions.values())[0][0])\n",
    "            with open(filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "                task_results = json.load(f)\n",
    "            for i in range(1, 3):\n",
    "                metrics[f'vote_{i}'] = evaluate(ground_truth, select_most_voted_solutions_solving_ties_with_logprob(task_results, i), verbose=False)[0]['pass_n']\n",
    "\n",
    "            print_metrics(metrics)\n",
    "            all_metrics[filepath] = {key: value for key, value in metrics.items() if key != 'max_correct_pixels' and key != 'any_correct_size'}\n",
    "        print()\n",
    "    return pd.DataFrame(all_metrics).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "data = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(data, solutions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# python inference.py --max_predictions_per_task=8 --output_filepath=submission_10_b.json --n_tasks=10\n",
    "# 10 tasks, 8 predictions, 7m17s\n",
    "accuracy: 0.0%\tcorrect_pixels: 71.3%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 1.2%\n",
    "\n",
    "# updated implementation, group prompts of task: 1m07s (x7 faster)\n",
    "accuracy: 0.0%\tcorrect_pixels: 71.2%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 1.2%\n",
    "\n",
    "# even faster implementation by grouping all prompts: 24s (x18 faster)\n",
    "accuracy: 0.0%\tcorrect_pixels: 71.3%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 1.2%\n",
    "# permute train samples\n",
    "accuracy: 0.0%\tcorrect_pixels: 69.9%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 2.5%\n",
    "\n",
    "# Same but on all the tasks, 7min\n",
    "accuracy: 4.4%\tcorrect_pixels: 77.0%\tcorrect_size: 88.0%\tpass_n: 12.0%\tunanswered: 2.6%\n",
    "accuracy: 4.4%\tcorrect_pixels: 77.1%\tcorrect_size: 88.0%\tpass_n: 12.0%\tunanswered: 2.6%\n",
    "# with the latest implementation: 1m50\n",
    "accuracy: 4.4%\tcorrect_pixels: 77.1%\tcorrect_size: 88.0%\tpass_n: 12.0%\tunanswered: 2.6%\n",
    "# with train sample permutation\n",
    "accuracy: 4.1%\tcorrect_pixels: 79.0%\tcorrect_size: 90.0%\tpass_n: 11.0%\tunanswered: 2.8%\n",
    "# with color augmentation and train sample permutation (this shows that data augmentation is not harmful)\n",
    "accuracy: 4.9%\tcorrect_pixels: 78.3%\tcorrect_size: 90.0%\tpass_n: 16.0%\tunanswered: 2.4%\n",
    "# with x4 times more augmentation (32 predictions per sample), 8min\n",
    "accuracy: 4.9%\tcorrect_pixels: 83.9%\tcorrect_size: 92.0%\tpass_n: 23.0%\tunanswered: 2.7%\n",
    "# with x16 more augmentation (128 predictions per sample), 34min\n",
    "accuracy: 4.6%\tcorrect_pixels: 85.7%\tcorrect_size: 94.0%\tpass_n: 23.5%\tunanswered: 2.5%\n",
    "# with 512 predictions per sample, around 3h30\n",
    "accuracy: 4.5%\tcorrect_pixels: 87.8%\tcorrect_size: 95.0%\tpass_n: 26.0%\tunanswered: 2.7%\n",
    "\n",
    "#beam search with best_of=2, 50min\n",
    "accuracy: 5.9%\tcorrect_pixels: 78.7%\tcorrect_size: 90.0%\tpass_n: 13.5%\tunanswered: 2.8%\n",
    "#beam search with best_of=4, 1h30\n",
    "accuracy: 6.0%\tcorrect_pixels: 77.5%\tcorrect_size: 89.0%\tpass_n: 13.5%\tunanswered: 2.8%\n",
    "#beam search with best_of=8\n",
    "CUDA error: an illegal memory access was encountered\n",
    "\n",
    "#qwen 1.5 32 predictions per sample, 12 min, submission_qwen15_x32.json\n",
    "accuracy: 4.9%\tcorrect_pixels: 83.7%\tcorrect_size: 92.0%\tpass_n: 21.0%\tunanswered: 3.4%\n",
    "#qwen 1.5 128 predictions per sample, 55 min\n",
    "accuracy: 5.0%\tcorrect_pixels: 86.0%\tcorrect_size: 93.0%\tpass_n: 25.0%\tunanswered: 3.4%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_effect_of_the_number_of_solutions(solutions, data, n_tries=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(data, select_most_voted_solutions(solutions, 2), verbose=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_attempt_accuracy(solutions, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the unanswered dissapear.\n",
    "\n",
    "```\n",
    "# before fixing voting bug\n",
    "accuracy: 6.2%\tcorrect_pixels: 72.6%\tcorrect_size: 85.5%\tpass_n: 12.5%\tunanswered: 4.0%\n",
    "Attempt 1 accuracy: 7.5%\tcorrect_pixels: 68.8%\tcorrect_size: 83.0%\tpass_n: 7.5%\tunanswered: 6.0%\t\n",
    "Attempt 2 accuracy: 5.0%\tcorrect_pixels: 66.0%\tcorrect_size: 81.0%\tpass_n: 5.0%\tunanswered: 2.0%\t\n",
    "\n",
    "#after fixing voting bug\n",
    "accuracy: 6.2%\tcorrect_pixels: 72.6%\tcorrect_size: 85.5%\tpass_n: 12.5%\tunanswered: 0.0%\n",
    "Attempt 1 accuracy: 7.5%\tcorrect_pixels: 69.7%\tcorrect_size: 84.0%\tpass_n: 7.5%\tunanswered: 0.0%\n",
    "Attempt 2 accuracy: 5.0%\tcorrect_pixels: 65.9%\tcorrect_size: 81.0%\tpass_n: 5.0%\tunanswered: 0.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tasks_and_predictions(solutions, data, only_correct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_analysis(model):\n",
    "    metrics, temperature = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_{model}_x128_t*.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        temperature.append(float(filepath.split('_t')[1].split('.json')[0])/10)\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.plot(temperature, [m[key] for m in metrics], 'o-')\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('temperature')\n",
    "    plt.suptitle(f'Effect of the temperature on the model {model}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_analysis(model='qwen05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_analysis(model='qwen15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study influence of output grid shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "data = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "global_metrics, task_metrics = evaluate(data, solutions, verbose=False)\n",
    "task_ids = list(task_metrics.keys())\n",
    "global_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_grid_shape(data):\n",
    "    shapes = dict()\n",
    "    for task_id, task in data.items():\n",
    "        output_shapes = [np.array(sample['output']).shape for sample in task['test'] + task['train']]\n",
    "        shapes[task_id] = np.mean(output_shapes, axis=0)\n",
    "    return shapes\n",
    "\n",
    "output_shapes = get_output_grid_shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(global_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'pass_n', 'unanswered'] #'correct_size', \n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(global_metrics), plot_idx + 1)\n",
    "    x = [output_shapes[task_id][1] for task_id in task_ids]\n",
    "    y = [output_shapes[task_id][0] for task_id in task_ids]\n",
    "    c = [task_metrics[task_id][key] for task_id in task_ids]\n",
    "    plt.scatter(x, y, c=c, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title(key)\n",
    "    plt.xlabel('cols')\n",
    "    plt.ylabel('rows')\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the output shape on the model performance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo beam-search analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "\n",
    "def pseudo_beam_search_analysis_v1():\n",
    "    metrics, n = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x8_T01_n*.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        n.append(int(filepath.split('_n')[1].split('.json')[0]))\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.scatter(n, [m[key] for m in metrics], )\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('n')\n",
    "    # plt.suptitle(f'Effect of the temperature on the model {model}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "pseudo_beam_search_analysis_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear result, let's do another analysis with different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "def pseudo_beam_search_analysis_v2(x):\n",
    "    metrics, temperature = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x{x}_n20_T??.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        print(filepath)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        temperature.append(float(filepath.split('_T')[1].split('.json')[0])/10)\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.plot(temperature, [m[key] for m in metrics], 'o-')\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('temperature')\n",
    "    plt.suptitle(f'Effect of the temperature on the model for {x} predictions')\n",
    "    plt.tight_layout()\n",
    "\n",
    "pseudo_beam_search_analysis_v2(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_beam_search_analysis_v2(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_n20_T08.json', 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "print(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15_x128.json', 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "print(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation loss vs metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "filepaths = sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/*/*/inference*.json'))\n",
    "filepaths = [filepath for filepath in filepaths if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')]\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    print(filepath)\n",
    "    metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "    print_metrics(metrics)\n",
    "    # metrics = evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]\n",
    "    # print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "filepaths = sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/06*/*/inference*.json'))\n",
    "filepaths = [filepath for filepath in filepaths if not filepath.endswith('voting.json') and not filepath.endswith('task_results.json')][:-1]\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    print(filepath)\n",
    "    metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "    print_metrics(metrics)\n",
    "    # metrics = evaluate(ground_truth, select_most_voted_solutions(solutions, 2), verbose=False)[0]\n",
    "    # print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"checkpoint_steps\": [500, 1000, 2000, 2650, 3000, 3450, 4000, 4850, 6000],\n",
    "    \"accuracy\": [0.4, 0.6, 1.1, 1.1, 1.3, 1.4, 2.2, 2.3, 2.3],\n",
    "    \"correct_pixels\": [58.3, 62.1, 63.8, 64.2, 65.0, 65.3, 65.4, 65.8, 66.7],\n",
    "    \"max_correct_pixels\": [76.7, 76.5, 80.6, 80.6, 80.5, 82.4, 83.0, 81.6, 81.7],\n",
    "    \"correct_size\": [78.0, 81.1, 83.4, 82.4, 84.0, 84.2, 84.3, 84.2, 84.9],\n",
    "    \"any_correct_size\": [88.5, 89.0, 90.0, 90.5, 90.0, 92.0, 92.0, 90.0, 89.5],\n",
    "    \"pass_n\": [6.5, 5.5, 10.0, 11.5, 10.5, 11.0, 15.0, 15.5, 15.0],\n",
    "    \"unanswered\": [7.1, 5.1, 5.6, 5.7, 4.3, 4.3, 4.4, 4.8, 4.1],\n",
    "    'val_loss': [0.237, 0.198, 0.169, 0.171, 0.162, 0.162, 0.159, 0.148, 0.159]\n",
    "}\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'correct_size', 'unanswered']\n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(keys), plot_idx + 1)\n",
    "    plt.plot(metrics['checkpoint_steps'], metrics[key], 'o-', label=key)\n",
    "    plt.grid()\n",
    "    plt.xlabel('checkpoint_steps')\n",
    "    plt.ylabel(key)\n",
    "plt.suptitle('Effect of the number of checkpoint steps on the model performance')\n",
    "plt.show()\n",
    "\n",
    "keys = ['pass_n', 'any_correct_size', 'max_correct_pixels', 'val_loss']\n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(keys), plot_idx + 1)\n",
    "    plt.plot(metrics['checkpoint_steps'], metrics[key], 'o-', label=key)\n",
    "    plt.grid()\n",
    "    plt.xlabel('checkpoint_steps')\n",
    "    plt.ylabel(key)\n",
    "plt.suptitle('Effect of the number of checkpoint steps on the model performance')\n",
    "plt.show()\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'correct_size', 'unanswered']\n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(keys), plot_idx + 1)\n",
    "    plt.plot(metrics['val_loss'], metrics[key], 'o-', label=key)\n",
    "    plt.grid()\n",
    "    plt.xlabel('val_loss')\n",
    "    plt.ylabel(key)\n",
    "plt.suptitle('Effect of the number of checkpoint steps on the model performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate multiple checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "#/mnt/hdd0/Kaggle/arc24/evaluations/20240826_grid_encoders/04_row-number-and-grid-shape_Qwen2-0.5B-Instruct_lr1e-4_r32_6e3steps/checkpoint-6000/inference.json\n",
    "#accuracy: 2.8%\tcorrect_pixels: 66.3%\tmax_correct_pixels: 82.2%\tcorrect_size: 84.2%\tany_correct_size: 91.0%\tpass_n: 18.5%\tunanswered: 2.8%\n",
    "#accuracy: 3.5%\tcorrect_pixels: 67.4%\tmax_correct_pixels: 72.2%\tcorrect_size: 85.4%\tany_correct_size: 87.9%\tpass_n: 7.1%\tunanswered: 2.5%\t\n",
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240828_grid_encoders_ttft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240901_data_scaling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240902_external_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240905_external_data_v2')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['group'] = [x.split('/')[-3].replace('msl_c', 'msl').replace('msl_b', 'msl') for x in df.index]\n",
    "df[df.n == 32].groupby('group').mean().style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240910_predict_inputs')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluate_multiple_checkpoints('/mnt/hdd0/Kaggle/arc24/evaluations/20240903_submission_models')\n",
    "df.style.format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Analyze number of succesfull predictions per task, that is the unanswered metric!\n",
    "- [x] How the number of predictions affects the metrics\n",
    "- [x] Sort the tasks by accuracy, correct pixels and correct size\n",
    "- [x] Visualize the tasks, sorted by accuracy\n",
    "- [x] Visualize the effect of grid shape in the metrics\n",
    "- [x] Accuracy of each attempt\n",
    "- [x] Dynamically choose ground truth based on number of predicted tasks (There's no need)\n",
    "- [x] Show vote_2 metric always\n",
    "- [x] Also show vote_1 metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
