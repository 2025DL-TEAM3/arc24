{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take the inference of a model and evaluate and visualize it.\n",
    "\n",
    "This will help to:\n",
    "\n",
    "- understand the failures of the model\n",
    "- find a better way to combine the model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_x512.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15_x128_voting.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_t01.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x128_t08.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen15.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# add path to python path\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions, evaluate,\n",
    "    study_effect_of_the_number_of_solutions,\n",
    "    study_attempt_accuracy,\n",
    "    visualize_tasks_and_predictions)\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "data = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(data, solutions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# python inference.py --max_predictions_per_task=8 --output_filepath=submission_10_b.json --n_tasks=10\n",
    "# 10 tasks, 8 predictions, 7m17s\n",
    "accuracy: 0.0%\tcorrect_pixels: 71.3%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 1.2%\n",
    "\n",
    "# updated implementation, group prompts of task: 1m07s (x7 faster)\n",
    "accuracy: 0.0%\tcorrect_pixels: 71.2%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 1.2%\n",
    "\n",
    "# even faster implementation by grouping all prompts: 24s (x18 faster)\n",
    "accuracy: 0.0%\tcorrect_pixels: 71.3%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 1.2%\n",
    "# permute train samples\n",
    "accuracy: 0.0%\tcorrect_pixels: 69.9%\tcorrect_size: 80.0%\tpass_n: 0.0%\tunanswered: 2.5%\n",
    "\n",
    "# Same but on all the tasks, 7min\n",
    "accuracy: 4.4%\tcorrect_pixels: 77.0%\tcorrect_size: 88.0%\tpass_n: 12.0%\tunanswered: 2.6%\n",
    "accuracy: 4.4%\tcorrect_pixels: 77.1%\tcorrect_size: 88.0%\tpass_n: 12.0%\tunanswered: 2.6%\n",
    "# with the latest implementation: 1m50\n",
    "accuracy: 4.4%\tcorrect_pixels: 77.1%\tcorrect_size: 88.0%\tpass_n: 12.0%\tunanswered: 2.6%\n",
    "# with train sample permutation\n",
    "accuracy: 4.1%\tcorrect_pixels: 79.0%\tcorrect_size: 90.0%\tpass_n: 11.0%\tunanswered: 2.8%\n",
    "# with color augmentation and train sample permutation (this shows that data augmentation is not harmful)\n",
    "accuracy: 4.9%\tcorrect_pixels: 78.3%\tcorrect_size: 90.0%\tpass_n: 16.0%\tunanswered: 2.4%\n",
    "# with x4 times more augmentation (32 predictions per sample), 8min\n",
    "accuracy: 4.9%\tcorrect_pixels: 83.9%\tcorrect_size: 92.0%\tpass_n: 23.0%\tunanswered: 2.7%\n",
    "# with x16 more augmentation (128 predictions per sample), 34min\n",
    "accuracy: 4.6%\tcorrect_pixels: 85.7%\tcorrect_size: 94.0%\tpass_n: 23.5%\tunanswered: 2.5%\n",
    "# with 512 predictions per sample, around 3h30\n",
    "accuracy: 4.5%\tcorrect_pixels: 87.8%\tcorrect_size: 95.0%\tpass_n: 26.0%\tunanswered: 2.7%\n",
    "\n",
    "#beam search with best_of=2, 50min\n",
    "accuracy: 5.9%\tcorrect_pixels: 78.7%\tcorrect_size: 90.0%\tpass_n: 13.5%\tunanswered: 2.8%\n",
    "#beam search with best_of=4, 1h30\n",
    "accuracy: 6.0%\tcorrect_pixels: 77.5%\tcorrect_size: 89.0%\tpass_n: 13.5%\tunanswered: 2.8%\n",
    "#beam search with best_of=8\n",
    "CUDA error: an illegal memory access was encountered\n",
    "\n",
    "#qwen 1.5 32 predictions per sample, 12 min, submission_qwen15_x32.json\n",
    "accuracy: 4.9%\tcorrect_pixels: 83.7%\tcorrect_size: 92.0%\tpass_n: 21.0%\tunanswered: 3.4%\n",
    "#qwen 1.5 128 predictions per sample, 55 min\n",
    "accuracy: 5.0%\tcorrect_pixels: 86.0%\tcorrect_size: 93.0%\tpass_n: 25.0%\tunanswered: 3.4%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_effect_of_the_number_of_solutions(solutions, data, n_tries=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_attempt_accuracy(solutions, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the unanswered dissapear.\n",
    "\n",
    "```\n",
    "# before fixing voting bug\n",
    "accuracy: 6.2%\tcorrect_pixels: 72.6%\tcorrect_size: 85.5%\tpass_n: 12.5%\tunanswered: 4.0%\n",
    "Attempt 1 accuracy: 7.5%\tcorrect_pixels: 68.8%\tcorrect_size: 83.0%\tpass_n: 7.5%\tunanswered: 6.0%\t\n",
    "Attempt 2 accuracy: 5.0%\tcorrect_pixels: 66.0%\tcorrect_size: 81.0%\tpass_n: 5.0%\tunanswered: 2.0%\t\n",
    "\n",
    "#after fixing voting bug\n",
    "accuracy: 6.2%\tcorrect_pixels: 72.6%\tcorrect_size: 85.5%\tpass_n: 12.5%\tunanswered: 0.0%\n",
    "Attempt 1 accuracy: 7.5%\tcorrect_pixels: 69.7%\tcorrect_size: 84.0%\tpass_n: 7.5%\tunanswered: 0.0%\n",
    "Attempt 2 accuracy: 5.0%\tcorrect_pixels: 65.9%\tcorrect_size: 81.0%\tpass_n: 5.0%\tunanswered: 0.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tasks_and_predictions(solutions, data, only_correct=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_analysis(model):\n",
    "    metrics, temperature = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_{model}_x128_t*.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        temperature.append(float(filepath.split('_t')[1].split('.json')[0])/10)\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.plot(temperature, [m[key] for m in metrics], 'o-')\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('temperature')\n",
    "    plt.suptitle(f'Effect of the temperature on the model {model}')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_analysis(model='qwen05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_analysis(model='qwen15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study influence of output grid shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "data = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "global_metrics, task_metrics = evaluate(data, solutions, verbose=False)\n",
    "task_ids = list(task_metrics.keys())\n",
    "global_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_grid_shape(data):\n",
    "    shapes = dict()\n",
    "    for task_id, task in data.items():\n",
    "        output_shapes = [np.array(sample['output']).shape for sample in task['test'] + task['train']]\n",
    "        shapes[task_id] = np.mean(output_shapes, axis=0)\n",
    "    return shapes\n",
    "\n",
    "output_shapes = get_output_grid_shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(global_metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "\n",
    "keys = ['accuracy', 'correct_pixels', 'pass_n', 'unanswered'] #'correct_size', \n",
    "for plot_idx, key in enumerate(keys):\n",
    "    plt.subplot(1, len(global_metrics), plot_idx + 1)\n",
    "    x = [output_shapes[task_id][1] for task_id in task_ids]\n",
    "    y = [output_shapes[task_id][0] for task_id in task_ids]\n",
    "    c = [task_metrics[task_id][key] for task_id in task_ids]\n",
    "    plt.scatter(x, y, c=c, cmap='viridis', alpha=0.5)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.title(key)\n",
    "    plt.xlabel('cols')\n",
    "    plt.ylabel('rows')\n",
    "    plt.xlim(0)\n",
    "    plt.ylim(0)\n",
    "    plt.grid()\n",
    "plt.suptitle('Effect of the output shape on the model performance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo beam-search analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "\n",
    "def pseudo_beam_search_analysis_v1():\n",
    "    metrics, n = [], []\n",
    "    for filepath in tqdm(sorted(glob.glob(f'/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_qwen05_x8_T01_n*.json'))):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions = json.load(f)\n",
    "        metrics.append(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "        n.append(int(filepath.split('_n')[1].split('.json')[0]))\n",
    "\n",
    "    keys = ['accuracy', 'pass_n', 'unanswered']\n",
    "    for plot_idx, key in enumerate(keys):\n",
    "        plt.subplot(1, len(keys), plot_idx + 1)\n",
    "        plt.scatter(n, [m[key] for m in metrics], )\n",
    "        plt.title(key)\n",
    "        plt.grid()\n",
    "        plt.xlabel('n')\n",
    "    # plt.suptitle(f'Effect of the temperature on the model {model}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "pseudo_beam_search_analysis_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No clear result, let's do another analysis with different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_qwen05_x8_T01_n8.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Analyze number of succesfull predictions per task, that is the unanswered metric!\n",
    "- [x] How the number of predictions affects the metrics\n",
    "- [x] Sort the tasks by accuracy, correct pixels and correct size\n",
    "- [x] Visualize the tasks, sorted by accuracy\n",
    "- [ ] Visualize the effect of grid shape in the metrics\n",
    "- [x] Accuracy of each attempt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
