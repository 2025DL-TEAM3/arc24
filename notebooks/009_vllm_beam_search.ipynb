{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam search with VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how beam search works with VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import textwrap\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_path: str = \"/home/gbarbadillo/data/Qwen2-1.5B-Instruct\"\n",
    "    max_model_len: int = 8192 #61000 for phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=cfg.model_path,\n",
    "            trust_remote_code=True,\n",
    "            dtype='half',\n",
    "            tensor_parallel_size=2, # to use 2 gpus\n",
    "            max_model_len=cfg.max_model_len,\n",
    "            #kv_cache_dtype='fp8_e5m2', I have disabled kv cache quantization because it is hurtful\n",
    "            enforce_eager=True, # without this 13.9GB of memory is used on each GPU, with this is 13.3GB,\n",
    "            disable_log_stats=True,\n",
    "            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt about Alexander the great and Tyre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use beam search we are able to generate text that has higher probability. The text changes.\n",
    "\n",
    "```\n",
    "greedy decoding\n",
    "Cumulative logprob: -22.31\n",
    "Alexander the Great was a legendary conqueror who ruled over a vast empire that\n",
    "stretched from Greece to India. One of his most famous conquests was the city of\n",
    "Tyre, which was located on the coast of Lebanon. Alexander had heard of Tyre\n",
    "\n",
    "beam search, best_of 100\n",
    "Cumulative logprob: -16.08\n",
    "Alexander the Great was one of the most famous conquerors in history. He was\n",
    "born in 356 BC and died in 323 BC, but his legacy lives on to this day. One of\n",
    "his most famous conquests was the\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_experiment():\n",
    "    logprob, runtime = [], []\n",
    "    best_of_range = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    for n in best_of_range:\n",
    "        if n == 1:\n",
    "            sampling_params = SamplingParams(temperature=0.0, max_tokens=max_tokens)\n",
    "        else:\n",
    "            sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=max_tokens, use_beam_search=True, best_of=n)\n",
    "        t0 = time.time()\n",
    "        ret = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "        logprob.append(ret[0].outputs[0].cumulative_logprob)\n",
    "        runtime.append(time.time() - t0)\n",
    "        print(f'## Best of {n} (logprob: {logprob[-1]:.2f}, runtime: {runtime[-1]:.2f}s)')\n",
    "        print(textwrap.fill(ret[0].outputs[0].text, 80) + '\\n\\n')\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_of_range, logprob, label='logprob', marker='o')\n",
    "    plt.xlabel('best_of')\n",
    "    plt.ylabel('Logprob')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(which='both')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_of_range, runtime, label='runtime', marker='o')\n",
    "    plt.xlabel('best_of')\n",
    "    plt.ylabel('Runtime')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(which='both')\n",
    "\n",
    "    plt.suptitle(f'Effect of best_of when using beam search to generate {max_tokens} tokens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are a helpful AI assistant'\n",
    "prompt = 'Write a small story about how Alexander the great conquered the city of Tyre'\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt\n",
    "max_tokens = 100\n",
    "beam_search_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are a helpful AI assistant'\n",
    "prompt = 'Write a small story about how Alexander the great conquered the city of Tyre'\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt\n",
    "max_tokens = 200\n",
    "beam_search_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does batching speedup inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unrelated to the notebook, but I want to find if batching speedsup the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are a helpful AI assistant'\n",
    "prompt = 'Write a small story about how Alexander the great conquered the city of Tyre'\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt\n",
    "max_tokens = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_experiment(n):\n",
    "    sampling_params = SamplingParams(temperature=0.0, max_tokens=max_tokens)\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        ret = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "    print(f'## {n} inferences (runtime: {time.time() - t0:.2f}s)')\n",
    "\n",
    "    t0 = time.time()\n",
    "    ret = llm.generate([prompt]*n, sampling_params, use_tqdm=False)\n",
    "    print(f'## Batch size {n} (runtime: {time.time() - t0:.2f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_experiment(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_experiment(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_experiment(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of potential when batching predictions.\n",
    "\n",
    "```\n",
    "## 20 inferences (runtime: 98.38s)\n",
    "## Batch size 20 (runtime: 5.52s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_vllm_gpu_memory():\n",
    "    global llm\n",
    "    # https://github.com/vllm-project/vllm/issues/1908\n",
    "    from vllm.distributed.parallel_state import destroy_model_parallel, destroy_distributed_environment\n",
    "    import torch\n",
    "    import gc\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    del llm.llm_engine.model_executor\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "clear_vllm_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
