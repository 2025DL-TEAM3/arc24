{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select grids with VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I use VLLM to select the correct grid answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_path: str = \"/home/gbarbadillo/data/Qwen2-0.5B-arc\"\n",
    "    max_model_len: int = 8192 #61000 for phi-3\n",
    "    solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_x512.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate\n",
    ")\n",
    "from inference import (\n",
    "    clear_vllm_gpu_memory,\n",
    "    SimplePromptCreator,\n",
    "    GridCodeBlockEncoder,\n",
    "    MinimalGridEncoder\n",
    ")\n",
    "from arc24.prompting import (\n",
    "    pretty_print_prompt,\n",
    "    system_prompt,\n",
    "    prompt_template,\n",
    "    answer_template,\n",
    "    remove_assistant_ending\n",
    ")\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=cfg.model_path,\n",
    "            trust_remote_code=True,\n",
    "            dtype='half',\n",
    "            tensor_parallel_size=2, # to use 2 gpus\n",
    "            max_model_len=cfg.max_model_len,\n",
    "            #kv_cache_dtype='fp8_e5m2', I have disabled kv cache quantization because it is hurtful\n",
    "            enforce_eager=True, # without this 13.9GB of memory is used on each GPU, with this is 13.3GB,\n",
    "            disable_log_stats=True,\n",
    "            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_encoder = GridCodeBlockEncoder(MinimalGridEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(task, test_idx, grid):\n",
    "    train_samples = [{key: grid_encoder.to_text(grid) for key, grid in sample.items()} for sample in task['train']]\n",
    "    user_message = prompt_template.render(train_samples=train_samples,\n",
    "                                            test_input=grid_encoder.to_text(task['test'][test_idx]['input']))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": answer_template.render(test_output=grid_encoder.to_text(grid))}]\n",
    "    # TODO: add start of assistant reply\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    prompt = remove_assistant_ending(prompt, cfg.model_path)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = list(solutions.keys())[0]\n",
    "test_idx = 0\n",
    "prompt = create_prompt(ground_truth[task_id], test_idx=test_idx, grid=ground_truth[task_id]['test'][test_idx]['output'])\n",
    "print(len(tokenizer.tokenize(prompt)))\n",
    "pretty_print_prompt(prompt, default_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute likelihood of each prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Github issue: Add efficient interface for evaluating probabilities of fixed prompt-completion pairs](https://github.com/vllm-project/vllm/issues/5234)\n",
    "- [prompt_logprobs](https://docs.vllm.ai/en/latest/dev/sampling_params.html) â€“ Number of log probabilities to return per prompt token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attributes(obj, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the attributes of an object and their values.\n",
    "    \n",
    "    :param obj: The object to inspect.\n",
    "    :param indent: The current level of indentation (used for recursive calls).\n",
    "    \"\"\"\n",
    "    spacing = ' ' * indent\n",
    "    print(f\"{spacing}{obj.__class__.__name__}:\")\n",
    "\n",
    "    # Get the attributes of the object\n",
    "    for attr in dir(obj):\n",
    "        # Ignore special attributes/methods\n",
    "        if not attr.startswith('__') and not callable(getattr(obj, attr)):\n",
    "            value = getattr(obj, attr)\n",
    "            if hasattr(value, '__dict__'):\n",
    "                print(f\"{spacing}  {attr}:\")\n",
    "                print_attributes(value, indent + 4)\n",
    "            # elif isinstance(value, list):\n",
    "            #     print(f\"{spacing}  {attr}:\")\n",
    "            #     for item in value:\n",
    "            #         print(f\"{spacing}    -\")\n",
    "            #         print_attributes(item, indent + 4)\n",
    "            else:\n",
    "                value = str(value)[:100]\n",
    "                print(f\"{spacing}  {attr}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1)\n",
    "ret = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "print_attributes(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_attributes(ret[0].outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can generate a single extra token very fast, but if I require the `prompt_logprobs` I get an OOM error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1, prompt_logprobs=1)\n",
    "ret = llm.generate(prompt[:100], sampling_params, use_tqdm=False)\n",
    "print_attributes(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0].prompt_logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is able to work with a small prompt though, and it outputs what we need. However for a prompt length between 2000 and 3000 characters the OOM happens.\n",
    "\n",
    "Thus I can not use this feature with VLLM, I would have to create my own implementation if I really want to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1, prompt_logprobs=1)\n",
    "ret = llm.generate(prompt[:3000], sampling_params, use_tqdm=False)\n",
    "print_attributes(ret[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get likelihood of the generated output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can try to use the likelihood of the already generated outputs. It is not the same because the inputs for each generation will be different. But maybe is enough..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=10, logprobs=1)\n",
    "ret = llm.generate(prompt[:-9], sampling_params, use_tqdm=False)\n",
    "print_attributes(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_attributes(ret[0].outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe that I can simply save the `cumulative_logprob` and the number of generated tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=10, logprobs=0)\n",
    "ret = llm.generate(prompt[:-9], sampling_params, use_tqdm=False)\n",
    "print_attributes(ret[0].outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(cumulative_logprob=ret[0].outputs[0].cumulative_logprob, n_tokens=len(ret[0].outputs[0].token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could compute a mean cumulative_logprob to compare the different answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm.llm_engine.model_executor\n",
    "del llm\n",
    "clear_vllm_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Print all the elements in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
