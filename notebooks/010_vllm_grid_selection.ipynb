{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select grids with VLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I use VLLM to select the correct grid answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    model_path: str = \"/home/gbarbadillo/data/Qwen2-0.5B-arc\"\n",
    "    max_model_len: int = 8192 #61000 for phi-3\n",
    "    solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_x512.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate\n",
    ")\n",
    "from inference import (\n",
    "    clear_vllm_gpu_memory,\n",
    "    SimplePromptCreator,\n",
    "    GridCodeBlockEncoder,\n",
    "    MinimalGridEncoder\n",
    ")\n",
    "from arc24.prompting import (\n",
    "    pretty_print_prompt,\n",
    "    system_prompt,\n",
    "    prompt_template,\n",
    "    answer_template,\n",
    "    remove_assistant_ending\n",
    ")\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=cfg.model_path,\n",
    "            trust_remote_code=True,\n",
    "            dtype='half',\n",
    "            tensor_parallel_size=2, # to use 2 gpus\n",
    "            max_model_len=cfg.max_model_len,\n",
    "            #kv_cache_dtype='fp8_e5m2', I have disabled kv cache quantization because it is hurtful\n",
    "            enforce_eager=True, # without this 13.9GB of memory is used on each GPU, with this is 13.3GB,\n",
    "            disable_log_stats=True,\n",
    "            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_encoder = GridCodeBlockEncoder(MinimalGridEncoder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(task, test_idx, grid):\n",
    "    train_samples = [{key: grid_encoder.to_text(grid) for key, grid in sample.items()} for sample in task['train']]\n",
    "    user_message = prompt_template.render(train_samples=train_samples,\n",
    "                                            test_input=grid_encoder.to_text(task['test'][test_idx]['input']))\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": answer_template.render(test_output=grid_encoder.to_text(grid))}]\n",
    "    # TODO: add start of assistant reply\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    prompt = remove_assistant_ending(prompt, cfg.model_path)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = list(solutions.keys())[0]\n",
    "test_idx = 0\n",
    "prompt = create_prompt(ground_truth[task_id], test_idx=test_idx, grid=ground_truth[task_id]['test'][test_idx]['output'])\n",
    "print(len(tokenizer.tokenize(prompt)))\n",
    "pretty_print_prompt(prompt, default_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute likelihood of each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1)\n",
    "ret = llm.generate(prompt, sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1, logprobs=1)\n",
    "ret = llm.generate(prompt, sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1, logprobs=1, prompt_logprobs=1)\n",
    "ret = llm.generate(prompt, sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(n=1, temperature=0.0, max_tokens=1, prompt_logprobs=0)\n",
    "ret = llm.generate(prompt, sampling_params, use_tqdm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm.llm_engine.model_executor\n",
    "del llm\n",
    "clear_vllm_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Print all the elements in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
