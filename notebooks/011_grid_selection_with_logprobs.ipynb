{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid selection with logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I use VLLM to select the correct grid answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --output_filepath=submission_x8_logprob.json\n",
    "python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=32 --output_filepath=submission_x32_logprob.json\n",
    "python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=128 --output_filepath=submission_x128_logprob.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/evaluations/first_evaluations/submission_x128_logprob.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate,\n",
    "    plot_grid,\n",
    "    plot_task,\n",
    "    print_metrics\n",
    ")\n",
    "from voting import select_most_voted_solutions\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the pass_n metric of all the predictions, and the accuracy of voting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "print_metrics(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "voting_solutions = select_most_voted_solutions(solutions, 2)\n",
    "print_metrics(evaluate(ground_truth, voting_solutions, verbose=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use all predictions we get a pass_n=23.5%, if we vote two candidates we get 12.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the use of logprobs to select the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "    rich_solutions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = dict()\n",
    "for output in rich_solutions:\n",
    "    if output['grid']:\n",
    "        task_id = output['task_id']\n",
    "        idx = output['idx']\n",
    "        if task_id not in grids:\n",
    "            grids[task_id] = dict()\n",
    "        if idx not in grids[task_id]:\n",
    "            grids[task_id][idx] = list()\n",
    "        keys = ['grid', 'n_tokens', 'cumulative_logprob']\n",
    "        info = {k: output[k] for k in keys}\n",
    "        info['mean_cumulative_logprob'] = info['cumulative_logprob'] / info['n_tokens']\n",
    "        grids[task_id][idx].append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_solutions(data):\n",
    "    solutions = dict()\n",
    "    for task_id, task in data.items():\n",
    "        solutions[task_id] = [dict(attempt_1=[], attempt_2=[]) for _ in task['test']]\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 5\n",
    "task_metrics = evaluate(ground_truth, solutions, verbose=False)[1]\n",
    "relevant_tasks = {task_id for task_id in task_metrics if task_metrics[task_id]['pass_n'] == 1.0}\n",
    "#relevant_tasks = {}\n",
    "\n",
    "\n",
    "solutions = create_empty_solutions({key: ground_truth[key] for key in grids.keys()})\n",
    "chosen_metric = 'mean_cumulative_logprob' # 'mean_cumulative_logprob', cumulative_logprob\n",
    "for task_id, task in grids.items():\n",
    "    if task_id in relevant_tasks: plot_task(ground_truth[task_id]); plt.suptitle(task_id); plt.show()\n",
    "    for idx, outputs in task.items():\n",
    "        outputs = sorted(outputs, key=lambda x: x[chosen_metric], reverse=True)\n",
    "        if task_id in relevant_tasks:\n",
    "            for plot_idx, output in enumerate(outputs[:n_outputs]):\n",
    "                plt.subplot(1, n_outputs, plot_idx + 1)\n",
    "                plot_grid(output['grid'])\n",
    "                plt.title(f'{output[chosen_metric]:.2f}')\n",
    "            plt.show()\n",
    "        best_output = outputs[0]\n",
    "        solutions[task_id][idx] = dict(attempt_1=best_output['grid'])\n",
    "        for output in outputs[1:]:\n",
    "            if output['grid'] == best_output['grid']:\n",
    "                continue\n",
    "            else:\n",
    "                solutions[task_id][idx]['attempt_2'] = output['grid']\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(evaluate(ground_truth, solutions, verbose=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to visualize the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
