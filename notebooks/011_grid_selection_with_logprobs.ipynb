{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid selection with logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I use VLLM to select the correct grid answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=8 --output_filepath=submission_x8_logprob.json\n",
    "python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=32 --output_filepath=submission_x32_logprob.json\n",
    "python inference.py --model_path=/home/gbarbadillo/data/Qwen2-0.5B-arc --predictions_per_task=128 --output_filepath=submission_x128_logprob.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/evaluations/first_evaluations/submission_x8_logprob.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/evaluations/first_evaluations/submission_x32_logprob.json'\n",
    "    # solutions_filepath: str = '/mnt/hdd0/Kaggle/arc24/evaluations/first_evaluations/submission_x128_logprob.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate,\n",
    "    plot_grid,\n",
    "    plot_task,\n",
    "    print_metrics\n",
    ")\n",
    "from voting import select_most_voted_solutions\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the pass_n metric of all the predictions, and the accuracy of voting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "print_metrics(evaluate(ground_truth, solutions, verbose=False)[0])\n",
    "voting_solutions = select_most_voted_solutions(solutions, 2)\n",
    "print_metrics(evaluate(ground_truth, voting_solutions, verbose=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use all predictions we get a pass_n=23.5%, if we vote two candidates we get 12.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the use of logprobs to select the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "    rich_solutions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_solutions(data):\n",
    "    solutions = dict()\n",
    "    for task_id, task in data.items():\n",
    "        solutions[task_id] = [dict(attempt_1=[], attempt_2=[]) for _ in task['test']]\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = dict()\n",
    "for output in rich_solutions:\n",
    "    if output['grid']:\n",
    "        task_id = output['task_id']\n",
    "        idx = output['idx']\n",
    "        if task_id not in grids:\n",
    "            grids[task_id] = dict()\n",
    "        if idx not in grids[task_id]:\n",
    "            grids[task_id][idx] = list()\n",
    "        keys = ['grid', 'n_tokens', 'cumulative_logprob']\n",
    "        info = {k: output[k] for k in keys}\n",
    "        info['mean_cumulative_logprob'] = info['cumulative_logprob'] / info['n_tokens']\n",
    "        grids[task_id][idx].append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 5\n",
    "task_metrics = evaluate(ground_truth, solutions, verbose=False)[1]\n",
    "relevant_tasks = {task_id for task_id in task_metrics if task_metrics[task_id]['pass_n'] == 1.0}\n",
    "show_plots = False\n",
    "\n",
    "correct_positions = []\n",
    "logprob_solutions = create_empty_solutions({key: ground_truth[key] for key in grids.keys()})\n",
    "chosen_metric = 'mean_cumulative_logprob' # 'mean_cumulative_logprob', cumulative_logprob\n",
    "for task_id, task in grids.items():\n",
    "    if task_id in relevant_tasks and show_plots: plot_task(ground_truth[task_id]); plt.suptitle(task_id); plt.show()\n",
    "    for idx, outputs in task.items():\n",
    "        outputs = sorted(outputs, key=lambda x: x[chosen_metric], reverse=True)\n",
    "        if task_id in relevant_tasks:\n",
    "            if show_plots:\n",
    "                for plot_idx, output in enumerate(outputs[:n_outputs]):\n",
    "                    plt.subplot(1, n_outputs, plot_idx + 1)\n",
    "                    plot_grid(output['grid'])\n",
    "                    title = f'{output[chosen_metric]:.2f}'\n",
    "                    if output['grid'] == ground_truth[task_id]['test'][idx]['output']:\n",
    "                        title = f'Correct\\n{title}'\n",
    "                    plt.title(title)\n",
    "                plt.show()\n",
    "            for position, output in enumerate(outputs):\n",
    "                if output['grid'] == ground_truth[task_id]['test'][idx]['output']:\n",
    "                    correct_positions.append(position)\n",
    "                    break\n",
    "        best_output = outputs[0]\n",
    "        logprob_solutions[task_id][idx] = dict(attempt_1=best_output['grid'])\n",
    "        for output in outputs[1:]:\n",
    "            if output['grid'] == best_output['grid']:\n",
    "                continue\n",
    "            else:\n",
    "                logprob_solutions[task_id][idx]['attempt_2'] = output['grid']\n",
    "                break\n",
    "print(f'naive {chosen_metric} mean correct position: {np.mean(np.clip(correct_positions, 0, 10)):.1f} ({correct_positions})')\n",
    "print_metrics(evaluate(ground_truth, logprob_solutions, verbose=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "mean_cumulative_logprob mean correct position: 4.3 ([82, 4, 1, 0, 0, 40, 3, 5, 101, 25, 0, 29, 15, 0, 14, 0, 0, 0, 3, 0, 0, 80, 1, 6, 42, 0])\n",
    "accuracy: 5.4%\tcorrect_pixels: 68.4%\tmax_correct_pixels: 73.0%\tcorrect_size: 82.9%\tany_correct_size: 86.2%\tpass_n: 10.7%\tunanswered: 0.0%\n",
    "\n",
    "cumulative_logprob mean correct position: 4.4 ([83, 5, 1, 0, 0, 40, 3, 5, 101, 27, 0, 86, 15, 0, 14, 0, 0, 0, 3, 0, 0, 79, 1, 6, 42, 0])\n",
    "accuracy: 5.4%\tcorrect_pixels: 68.9%\tmax_correct_pixels: 73.8%\tcorrect_size: 83.9%\tany_correct_size: 87.2%\tpass_n: 10.7%\tunanswered: 0.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate logprobs for the same prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = dict()\n",
    "for output in rich_solutions:\n",
    "    if output['grid']:\n",
    "        task_id = output['task_id']\n",
    "        idx = output['idx']\n",
    "        if task_id not in grids:\n",
    "            grids[task_id] = dict()\n",
    "        if idx not in grids[task_id]:\n",
    "            grids[task_id][idx] = dict()\n",
    "        grid_key = str(output['grid'])\n",
    "        if grid_key not in grids[task_id][idx]:\n",
    "            grids[task_id][idx][grid_key] = dict(\n",
    "                grid=output['grid'], cumulative_logprob=[], mean_cumulative_logprob=[])\n",
    "        grids[task_id][idx][grid_key]['cumulative_logprob'].append(output['cumulative_logprob'])\n",
    "        grids[task_id][idx][grid_key]['mean_cumulative_logprob'].append(output['cumulative_logprob'] / output['n_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 5\n",
    "task_metrics = evaluate(ground_truth, solutions, verbose=False)[1]\n",
    "relevant_tasks = {task_id for task_id in task_metrics if task_metrics[task_id]['pass_n'] == 1.0}\n",
    "show_plots = False\n",
    "implementation = 'voting_mean_logprob'\n",
    "\n",
    "def get_title(output):\n",
    "    # if len(output[chosen_metric]) == 1:\n",
    "    #     title = f'{np.mean(output[chosen_metric]):.1e} ± {1.96*default_std:.0e} ({len(output[chosen_metric])})'\n",
    "    # else:\n",
    "    #     title = f'{np.mean(output[chosen_metric]):.1e} ± {1.96*np.std(output[chosen_metric])/np.sqrt(len(output[chosen_metric])):.0e} ({len(output[chosen_metric])})'\n",
    "    title = f'{np.mean(output[chosen_metric]):.1e} ± {1.96*default_std/np.sqrt(len(output[chosen_metric])):.0e} ({len(output[chosen_metric])})'\n",
    "    return title\n",
    "\n",
    "correct_positions = []\n",
    "logprob_solutions = create_empty_solutions({key: ground_truth[key] for key in grids.keys()})\n",
    "chosen_metric = 'cumulative_logprob' # 'mean_cumulative_logprob', cumulative_logprob\n",
    "for task_id, task in grids.items():\n",
    "    if task_id in relevant_tasks and show_plots: plot_task(ground_truth[task_id]); plt.suptitle(task_id); plt.show()\n",
    "    for idx, outputs in task.items():\n",
    "        default_std = np.mean([np.std(output[chosen_metric]) for output in outputs.values() if len(output[chosen_metric]) > 1])\n",
    "        for output in outputs.values():\n",
    "            if implementation == 'voting':\n",
    "                output['ranking'] = len(output[chosen_metric])\n",
    "            elif implementation == 'mean_logprob':\n",
    "                output['ranking'] = np.mean(output[chosen_metric])\n",
    "            elif implementation == 'lower_bound':\n",
    "                if len(output[chosen_metric]) == 1:\n",
    "                    output['ranking'] = output[chosen_metric][0] - 1.96*default_std\n",
    "                else:\n",
    "                    output['ranking'] = np.mean(output[chosen_metric]) - 1.96*np.std(output[chosen_metric])/np.sqrt(len(output[chosen_metric]))\n",
    "            elif implementation == 'lower_bound_constant_std':\n",
    "                output['ranking'] = np.mean(output[chosen_metric]) - 1.96*default_std/np.sqrt(len(output[chosen_metric]))\n",
    "\n",
    "            elif implementation == 'voting_mean_logprob':\n",
    "                output['ranking'] = (len(output[chosen_metric]), np.mean(output[chosen_metric]))\n",
    "            # This one does not have sense, the ties will only happen when the number of votes is the same, so the mean value is the only relevant metric\n",
    "            # if implementation == 'voting_lower_bound_constant_std':\n",
    "            #     output['ranking'] = (len(output[chosen_metric]), np.mean(output[chosen_metric]) - 1.96*default_std/np.sqrt(len(output[chosen_metric])))\n",
    "            else:\n",
    "                raise ValueError(f'Unknown implementation: {implementation}')\n",
    "        outputs = sorted(outputs.values(), key=lambda x: x['ranking'], reverse=True)\n",
    "        if task_id in relevant_tasks:\n",
    "            if show_plots:\n",
    "                plotted_correct_grid = False\n",
    "                for plot_idx, output in enumerate(outputs[:n_outputs]):\n",
    "                    plt.subplot(1, n_outputs, plot_idx + 1)\n",
    "                    plot_grid(output['grid'])\n",
    "                    title = get_title(output)\n",
    "                    if output['grid'] == ground_truth[task_id]['test'][idx]['output']:\n",
    "                        title = f'Correct\\n{title}'\n",
    "                        plotted_correct_grid = True\n",
    "                    plt.title(title)\n",
    "                if not plotted_correct_grid:\n",
    "                    for output in outputs:\n",
    "                        if output['grid'] == ground_truth[task_id]['test'][idx]['output']:\n",
    "                            plt.subplot(1, n_outputs, n_outputs)\n",
    "                            plot_grid(output['grid'])\n",
    "                            title = get_title(output)\n",
    "                            title = f'Correct\\n{title}'\n",
    "                            plt.title(title)\n",
    "                            break\n",
    "                plt.show()\n",
    "            for position, output in enumerate(outputs):\n",
    "                if output['grid'] == ground_truth[task_id]['test'][idx]['output']:\n",
    "                    correct_positions.append(position)\n",
    "                    break\n",
    "        best_output = outputs[0]\n",
    "        logprob_solutions[task_id][idx] = dict(attempt_1=best_output['grid'])\n",
    "        for output in outputs[1:]:\n",
    "            if output['grid'] == best_output['grid']:\n",
    "                continue\n",
    "            else:\n",
    "                logprob_solutions[task_id][idx]['attempt_2'] = output['grid']\n",
    "                break\n",
    "print(f'{implementation} {chosen_metric} mean correct position: {np.mean(np.clip(correct_positions, 0, 10)):.1f} ({correct_positions})')\n",
    "print_metrics(evaluate(ground_truth, logprob_solutions, verbose=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "voting_mean_logprob mean_cumulative_logprob mean correct position: 2.2 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 27, 1, 3, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n",
    "voting_lower_bound_constant_std mean_cumulative_logprob mean correct position: 2.2 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 27, 1, 3, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n",
    "\n",
    "voting_mean_logprob cumulative_logprob mean correct position: 2.3 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 29, 1, 4, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n",
    "voting_lower_bound_constant_std cumulative_logprob mean correct position: 2.3 ([1, 5, 1, 0, 0, 3, 1, 0, 3, 29, 1, 4, 11, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n",
    "\n",
    "there is no difference here.\n",
    "\n",
    "\n",
    "voting mean_cumulative_logprob mean correct position: 2.4 ([1, 7, 1, 0, 0, 3, 1, 0, 4, 35, 1, 4, 14, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n",
    "voting cumulative_logprob mean correct position: 2.4 ([1, 7, 1, 0, 0, 3, 1, 0, 4, 35, 1, 4, 14, 0, 1, 0, 0, 0, 1, 0, 2, 3, 6, 3, 4, 0])\n",
    "\n",
    "lower_bound_constant_std cumulative_logprob mean correct position: 2.8      ([17, 5, 0, 0, 0, 2, 2, 1, 3, 24, 0, 16, 5, 0, 9, 0, 0, 0, 0, 0, 0, 7, 2, 3, 5, 0])\n",
    "lower_bound_constant_std mean_cumulative_logprob mean correct position: 2.8 ([14, 12, 0, 0, 0, 2, 2, 1, 3, 22, 0, 5, 5, 0, 9, 0, 0, 0, 0, 0, 0, 7, 2, 2, 5, 0])\n",
    "\n",
    "\n",
    "lower_bound mean_cumulative_logprob mean correct position: 3.0 ([15, 18, 0, 0, 0, 2, 2, 0, 3, 21, 0, 9, 3, 0, 9, 0, 0, 0, 0, 0, 1, 7, 5, 3, 5, 0])\n",
    "lower_bound cumulative_logprob mean correct position: 3.0 ([17, 11, 0, 0, 0, 2, 1, 0, 3, 24, 0, 17, 3, 0, 9, 0, 0, 0, 0, 0, 1, 7, 5, 3, 5, 0])\n",
    "\n",
    "\n",
    "mean_logprob cumulative_logprob mean correct position: 3.3 ([19, 6, 1, 0, 0, 1, 2, 5, 3, 22, 0, 17, 3, 0, 15, 2, 0, 0, 0, 0, 2, 11, 3, 3, 5, 0])\n",
    "mean_logprob mean_cumulative_logprob mean correct position: 3.3 ([20, 15, 1, 0, 0, 1, 2, 5, 3, 20, 0, 7, 3, 0, 15, 2, 0, 0, 0, 0, 2, 11, 3, 3, 5, 0])\n",
    "\n",
    "naive mean_cumulative_logprob mean correct position: 4.3 ([82, 4, 1, 0, 0, 40, 3, 5, 101, 25, 0, 29, 15, 0, 14, 0, 0, 0, 3, 0, 0, 80, 1, 6, 42, 0])\n",
    "naive cumulative_logprob mean correct position: 4.4 ([83, 5, 1, 0, 0, 40, 3, 5, 101, 27, 0, 86, 15, 0, 14, 0, 0, 0, 3, 0, 0, 79, 1, 6, 42, 0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Compute lower bound with shared standard deviation\n",
    "#cumulative_logprob\n",
    "accuracy: 5.4%\tcorrect_pixels: 67.6%\tmax_correct_pixels: 73.4%\tcorrect_size: 81.9%\tany_correct_size: 86.2%\tpass_n: 10.7%\tunanswered: 0.0%\t\n",
    "# mean_cumulative_logprob\n",
    "accuracy: 5.4%\tcorrect_pixels: 66.9%\tmax_correct_pixels: 73.1%\tcorrect_size: 80.9%\tany_correct_size: 86.2%\tpass_n: 10.7%\tunanswered: 0.0%\t\n",
    "\n",
    "# Compute lower bound\n",
    "#cumulative_logprob\n",
    "accuracy: 6.1%\tcorrect_pixels: 68.5%\tmax_correct_pixels: 73.8%\tcorrect_size: 82.4%\tany_correct_size: 86.2%\tpass_n: 12.2%\tunanswered: 0.0%\t\n",
    "# mean_cumulative_logprob\n",
    "accuracy: 5.6%\tcorrect_pixels: 68.4%\tmax_correct_pixels: 73.0%\tcorrect_size: 82.4%\tany_correct_size: 86.2%\tpass_n: 11.2%\tunanswered: 0.0%\t\n",
    "# First implementation\n",
    "# cumulative_logprob\n",
    "accuracy: 4.8%\tcorrect_pixels: 67.0%\tmax_correct_pixels: 72.8%\tcorrect_size: 81.4%\tany_correct_size: 85.2%\tpass_n: 9.7%\tunanswered: 0.0%\t\n",
    "# mean_cumulative_logprob\n",
    "accuracy: 4.8%\tcorrect_pixels: 65.6%\tmax_correct_pixels: 70.8%\tcorrect_size: 79.3%\tany_correct_size: 83.2%\tpass_n: 9.7%\tunanswered: 0.0%\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- [ ] How the number of predictions affects to the comparison, voting scales well with the number of predictions\n",
    "- [ ] Should I use other metrics such as mean position?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
