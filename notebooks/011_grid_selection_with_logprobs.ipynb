{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid selection with logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I use VLLM to select the correct grid answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    solutions_filepath: str = '/mnt/hdd0/MEGA/AI/22_Kaggle/arc24/scripts/submission_x8_logprob.json'\n",
    "    dataset_filepath: str = '/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate,\n",
    "    plot_grid,\n",
    "    plot_task\n",
    ")\n",
    "from inference import (\n",
    "    _create_empty_solutions\n",
    ")\n",
    "from arc24.prompting import (\n",
    "    pretty_print_prompt,\n",
    "    system_prompt,\n",
    "    prompt_template,\n",
    "    answer_template,\n",
    "    remove_assistant_ending\n",
    ")\n",
    "from voting import select_most_voted_solutions\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the pass_n metric of all the predictions, and the accuracy of voting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath, 'r') as f:\n",
    "    solutions = json.load(f)\n",
    "ground_truth = load_arc_data_with_solutions(cfg.dataset_filepath)\n",
    "evaluate(ground_truth, solutions, verbose=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_solutions = select_most_voted_solutions(solutions, 2)\n",
    "evaluate(ground_truth, voting_solutions, verbose=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use all predictions we get a pass_n=23.5%, if we vote two candidates we get 12.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the use of logprobs to select the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cfg.solutions_filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "    rich_solutions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = dict()\n",
    "task_metrics = evaluate(ground_truth, solutions, verbose=False)[1]\n",
    "relevant_tasks = {task_id for task_id in task_metrics if task_metrics[task_id]['pass_n'] == 1.0}\n",
    "for output in rich_solutions:\n",
    "    if output['grid']:\n",
    "        task_id = output['task_id']\n",
    "        if task_id not in relevant_tasks:\n",
    "            continue\n",
    "        idx = output['idx']\n",
    "        if task_id not in grids:\n",
    "            grids[task_id] = dict()\n",
    "        if idx not in grids[task_id]:\n",
    "            grids[task_id][idx] = list()\n",
    "        keys = ['grid', 'n_tokens', 'cumulative_logprob']\n",
    "        info = {k: output[k] for k in keys}\n",
    "        info['mean_cumulative_logprob'] = info['cumulative_logprob'] / info['n_tokens']\n",
    "        grids[task_id][idx].append(info)\n",
    "len(relevant_tasks), len(grids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_solutions(data):\n",
    "    solutions = dict()\n",
    "    for task_id, task in data.items():\n",
    "        solutions[task_id] = [dict(attempt_1=[], attempt_2=[]) for _ in task['test']]\n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outputs = 5\n",
    "\n",
    "solutions = create_empty_solutions({key: ground_truth[key] for key in grids.keys()})\n",
    "chosen_metric = 'cumulative_logprob'\n",
    "for task_id, task in grids.items():\n",
    "    plot_task(ground_truth[task_id]); plt.suptitle(task_id); plt.show()\n",
    "    for idx, outputs in task.items():\n",
    "        outputs = sorted(outputs, key=lambda x: x[chosen_metric], reverse=True)\n",
    "\n",
    "        for output in outputs[:n_outputs]:\n",
    "            plt.subplot(1, n_outputs, outputs.index(output) + 1)\n",
    "            plot_grid(output['grid'])\n",
    "            plt.title(f'{output[chosen_metric]:.2f}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print([output[chosen_metric] for output in outputs[:3]])\n",
    "        best_output = outputs[0]\n",
    "        solutions[task_id][idx] = dict(attempt_1=best_output['grid'])\n",
    "        for output in outputs[1:]:\n",
    "            if output['grid'] == best_output['grid']:\n",
    "                continue\n",
    "            else:\n",
    "                solutions[task_id][idx]['attempt_2'] = output['grid']\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, solutions, verbose=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to visualize the metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
