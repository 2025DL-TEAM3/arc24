{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I find alternative symbols to the numbers for the grids?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "from arc24.encoders import create_grid_encoder\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_symbol_unique(symbol, vocab):\n",
    "    occurrences = 0\n",
    "    for word in vocab:\n",
    "        if symbol in word:\n",
    "            occurrences += 1\n",
    "    return occurrences == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_symbol(symbol, vocab, skip_special_tokens=True):\n",
    "    words = [word for word in vocab if symbol in word]\n",
    "    if skip_special_tokens:\n",
    "        words = [word for word in words if not word.startswith('<')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_symbol_unique('ø', llama_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(word, idx) for word, idx in llama_vocab.items() if idx in [39218, 6282]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_symbol_unique('Ã¸', llama_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{word for word in llama_vocab if 'ø' in word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-0.5B-Instruct')\n",
    "qwen_vocab = qwen_tokenizer.get_vocab()\n",
    "qwen_length_1_words = sorted([word for word in qwen_vocab if len(word) == 1 and is_symbol_unique(word, qwen_vocab)])\n",
    "print(f'{len(qwen_length_1_words)}/{len(qwen_tokenizer.get_vocab())}')\n",
    "print(qwen_length_1_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/llama-3.1-transformers-8b-instruct-v1')\n",
    "llama_vocab = llama_tokenizer.get_vocab()\n",
    "llama_length_1_words = sorted([word for word in llama_vocab if len(word) == 1 and is_symbol_unique(word, llama_vocab)])\n",
    "print(f'{len(llama_length_1_words)}/{len(llama_tokenizer.get_vocab())}')\n",
    "print(llama_length_1_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the intersection of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_symbols = sorted(list(set(qwen_length_1_words).intersection(set(llama_length_1_words))))\n",
    "print(len(interesting_symbols))\n",
    "print(interesting_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have 44 symbols that are apparently unique both for llama and qwen. We could create a mapping between these symbols and the numbers.\n",
    "\n",
    "```\n",
    "['À', 'Á', 'ñ', 'ò', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ċ', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'ġ']\n",
    "```\n",
    "\n",
    "I'm going to select a different enough set, that could enable easy visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to verify that the encoding does not change even if distractors are added. When verifying the new representation I have found that spaces or new lines could change the encoding, making it more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_symbols = []\n",
    "for symbol in interesting_symbols:\n",
    "    add_to_candidates = True\n",
    "    for distractor in [' ', '\\n']:\n",
    "        text = distractor + symbol + distractor\n",
    "        if len(llama_tokenizer.tokenize(text)) != 3 or len(qwen_tokenizer.tokenize(text)) != 3:\n",
    "            add_to_candidates = False\n",
    "            break\n",
    "    if add_to_candidates:\n",
    "        candidate_symbols.append(symbol)\n",
    "candidate_symbols = sorted(candidate_symbols)\n",
    "print(len(candidate_symbols))\n",
    "print(candidate_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have 16 symbols that apparently do not change when being surrounded by spaces or new lines.\n",
    "\n",
    "```\n",
    "['ñ', 'ò', 'õ', '÷', 'ù', 'û', 'ā', 'Ă', 'ă', 'ą', 'ć', 'ď', 'ē', 'ę', 'Ě', 'Ğ']\n",
    "```\n",
    "\n",
    "Let's generate a big synthetic data to verify that is correctly encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['ñ', 'ò', 'õ', '÷', 'ù', 'û', 'ā', 'Ă', 'ă', 'ą', 'ć', 'ď', 'ē', 'ę', 'Ě', 'Ğ']\n",
    "selection = ['ñ', 'ò', '÷', 'û', 'ą', 'ć', 'ď', 'ę', 'Ě', 'Ğ']\n",
    "print(selection)\n",
    "print(len(selection))\n",
    "a, b = 20, 20\n",
    "text = '\\n'.join(' ' + ''.join(np.random.choice(selection, b, replace=True)) for _ in range(a))\n",
    "print(text)\n",
    "n = a*b + a - 1\n",
    "assert len(text) == len(llama_tokenizer.tokenize(text)), f'{n} != {len(llama_tokenizer.tokenize(text))}'\n",
    "assert len(text) == len(qwen_tokenizer.tokenize(text)), f'{n} != {len(qwen_tokenizer.tokenize(text))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [str(i) for i in range(10)]\n",
    "print(selection)\n",
    "a, b = 20, 20\n",
    "text = '\\n'.join(' ' + ''.join(np.random.choice(selection, b, replace=True)) for _ in range(a))\n",
    "print(text)\n",
    "n = a*(b+1) + a - 1\n",
    "assert len(text) == len(qwen_tokenizer.tokenize(text)), f'{n} != {len(qwen_tokenizer.tokenize(text))}'\n",
    "assert len(text) == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [str(i)*3 for i in range(10)]\n",
    "print(selection)\n",
    "a, b = 20, 20\n",
    "text = '\\n'.join(' ' + ''.join(np.random.choice(selection, b, replace=True)) for _ in range(a))\n",
    "print(text)\n",
    "n = a*(b+1) + a - 1\n",
    "assert n == len(llama_tokenizer.tokenize(text)), f'{n} != {len(llama_tokenizer.tokenize(text))}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's see the encoding on a real grid sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "```grid shape: 3x3\n",
    "1 ñò÷\n",
    "2 ûąć\n",
    "3 ďęĚ\n",
    "```\n",
    "\"\"\"\n",
    "print(qwen_tokenizer.tokenize(text))\n",
    "print(llama_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3.2 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/Llama-3.2-1B-Instruct')\n",
    "llama_vocab = llama_tokenizer.get_vocab()\n",
    "llama_length_1_words = sorted([word for word in llama_vocab if len(word) == 1 and is_symbol_unique(word, llama_vocab)])\n",
    "print(f'{len(llama_length_1_words)}/{len(llama_tokenizer.get_vocab())}')\n",
    "print(llama_length_1_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    words = sorted(get_words_with_symbol(str(i), llama_vocab))\n",
    "    print(f'{i} ({len(words)}): {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the tokenizer has all the number variations from 0 to 999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    words = sorted(get_words_with_symbol(str(i)*3, llama_vocab))\n",
    "    print(f'{i} ({len(words)}): {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could repeat each number 3 times to have one token per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(RepeatNumberEncoder()))')\n",
    "text = encoder.to_text(np.eye(3, dtype=int).tolist())\n",
    "print(text)\n",
    "llama_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoll-135M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/SmolLM-135M-Instruct')\n",
    "vocab = tokenizer.get_vocab()\n",
    "length_1_words = sorted([word for word in vocab if len(word) == 1 and is_symbol_unique(word, vocab)])\n",
    "print(f'{len(length_1_words)}/{len(tokenizer.get_vocab())}')\n",
    "print(length_1_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    words = sorted(get_words_with_symbol(str(i), vocab))\n",
    "    print(f'{i} ({len(words)}): {words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SmolLM I can encode the grid directly with numbers without any problem, just like Qwen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem with pad_token and eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/SmolLM-135M-Instruct')\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/Llama-3.2-1B-Instruct')\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/Qwen2.5-0.5B-Instruct')\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/Qwen2.5-0.5B')\n",
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Qwen2.5-0.5B` and `SmolLM-135M-Instruc` use the same eos and pad token, that is why generation with those models does not end.\n",
    "- In the other hand `Llama-3.2-1B-Instruct` does not even have a pad token, but I add it in the fine-tuning script\n",
    "- Qwen instruct models have different pad and eos tokens, which is the perfect situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_path, pad_token='<|pad|>'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        trust_remote_code=True)\n",
    "    if 'pad_token' not in tokenizer.special_tokens_map or tokenizer.pad_token == tokenizer.eos_token:\n",
    "        if 'pad_token' not in tokenizer.special_tokens_map:\n",
    "            print('Adding padding token because the tokenizer does not have one')\n",
    "        else:\n",
    "            print('Changing padding token because it is the same as the end-of-sequence token')\n",
    "        assert pad_token not in tokenizer.get_vocab()\n",
    "        tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "        tokenizer.padding_side = 'right'\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in ['/home/gbarbadillo/data/SmolLM-135M-Instruct', '/home/gbarbadillo/data/Llama-3.2-1B-Instruct', '/home/gbarbadillo/data/Qwen2.5-0.5B', '/home/gbarbadillo/data/Qwen2.5-0.5B-Instruct']:\n",
    "    print(filepath)\n",
    "    tokenizer = get_tokenizer(filepath)\n",
    "    print(tokenizer.special_tokens_map)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
