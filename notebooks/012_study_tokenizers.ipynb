{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can I find alternative symbols to the numbers for the grids?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_symbol_unique(symbol, vocab):\n",
    "    occurrences = 0\n",
    "    for word in vocab:\n",
    "        if symbol in word:\n",
    "            occurrences += 1\n",
    "    return occurrences == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_symbol_unique('ø', llama_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(word, idx) for word, idx in llama_vocab.items() if idx in [39218, 6282]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_symbol_unique('Ã¸', llama_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{word for word in llama_vocab if 'ø' in word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-0.5B-Instruct')\n",
    "qwen_vocab = qwen_tokenizer.get_vocab()\n",
    "qwen_length_1_words = sorted([word for word in qwen_vocab if len(word) == 1 and is_symbol_unique(word, qwen_vocab)])\n",
    "print(f'{len(qwen_length_1_words)}/{len(qwen_tokenizer.get_vocab())}')\n",
    "print(qwen_length_1_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained('/home/gbarbadillo/data/llama-3.1-transformers-8b-instruct-v1')\n",
    "llama_vocab = llama_tokenizer.get_vocab()\n",
    "llama_length_1_words = sorted([word for word in llama_vocab if len(word) == 1 and is_symbol_unique(word, llama_vocab)])\n",
    "print(f'{len(llama_length_1_words)}/{len(llama_tokenizer.get_vocab())}')\n",
    "print(llama_length_1_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the intersection of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_symbols = sorted(list(set(qwen_length_1_words).intersection(set(llama_length_1_words))))\n",
    "print(len(interesting_symbols))\n",
    "print(interesting_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have 44 symbols that are apparently unique both for llama and qwen. We could create a mapping between these symbols and the numbers.\n",
    "\n",
    "```\n",
    "['À', 'Á', 'ñ', 'ò', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ċ', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'ġ']\n",
    "```\n",
    "\n",
    "I'm going to select a different enough set, that could enable easy visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also going to verify that the encoding does not change even if distractors are added. When verifying the new representation I have found that spaces or new lines could change the encoding, making it more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_symbols = []\n",
    "for symbol in interesting_symbols:\n",
    "    add_to_candidates = True\n",
    "    for distractor in [' ', '\\n']:\n",
    "        text = distractor + symbol + distractor\n",
    "        if len(llama_tokenizer.tokenize(text)) != 3 or len(qwen_tokenizer.tokenize(text)) != 3:\n",
    "            add_to_candidates = False\n",
    "            break\n",
    "    if add_to_candidates:\n",
    "        candidate_symbols.append(symbol)\n",
    "candidate_symbols = sorted(candidate_symbols)\n",
    "print(len(candidate_symbols))\n",
    "print(candidate_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have 16 symbols that apparently do not change when being surrounded by spaces or new lines.\n",
    "\n",
    "```\n",
    "['ñ', 'ò', 'õ', '÷', 'ù', 'û', 'ā', 'Ă', 'ă', 'ą', 'ć', 'ď', 'ē', 'ę', 'Ě', 'Ğ']\n",
    "```\n",
    "\n",
    "Let's generate a big synthetic data to verify that is correctly encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = ['ñ', 'ò', 'õ', '÷', 'ù', 'û', 'ā', 'Ă', 'ă', 'ą', 'ć', 'ď', 'ē', 'ę', 'Ě', 'Ğ']\n",
    "selection = ['ñ', 'ò', '÷', 'û', 'ą', 'ć', 'ď', 'ę', 'Ě', 'Ğ']\n",
    "print(selection)\n",
    "print(len(selection))\n",
    "a, b = 20, 20\n",
    "text = '\\n'.join(' ' + ''.join(np.random.choice(selection, b, replace=True)) for _ in range(a))\n",
    "print(text)\n",
    "n = a*b + a - 1\n",
    "assert len(text) == len(llama_tokenizer.tokenize(text)), f'{n} != {len(llama_tokenizer.tokenize(text))}'\n",
    "assert len(text) == len(qwen_tokenizer.tokenize(text)), f'{n} != {len(qwen_tokenizer.tokenize(text))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [str(i) for i in range(10)]\n",
    "print(selection)\n",
    "a, b = 20, 20\n",
    "text = '\\n'.join(' ' + ''.join(np.random.choice(selection, b, replace=True)) for _ in range(a))\n",
    "print(text)\n",
    "n = a*(b+1) + a - 1\n",
    "assert len(text) == len(qwen_tokenizer.tokenize(text)), f'{n} != {len(qwen_tokenizer.tokenize(text))}'\n",
    "assert len(text) == n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [str(i)*3 for i in range(10)]\n",
    "print(selection)\n",
    "a, b = 20, 20\n",
    "text = '\\n'.join(' ' + ''.join(np.random.choice(selection, b, replace=True)) for _ in range(a))\n",
    "print(text)\n",
    "n = a*(b+1) + a - 1\n",
    "assert n == len(llama_tokenizer.tokenize(text)), f'{n} != {len(llama_tokenizer.tokenize(text))}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's see the encoding on a real grid sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "```grid shape: 3x3\n",
    "1 ñò÷\n",
    "2 ûąć\n",
    "3 ďęĚ\n",
    "```\n",
    "\"\"\"\n",
    "print(qwen_tokenizer.tokenize(text))\n",
    "print(llama_tokenizer.tokenize(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
