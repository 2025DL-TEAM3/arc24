{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we can use a fine-tuned LLM to generate new inputs for the ARC tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    base_model_path: str = '/home/gbarbadillo/data/Qwen2-0.5B-Instruct'\n",
    "    # model_checkpoint: str = '/mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/01_baseline/checkpoint-1000'\n",
    "    # model_checkpoint: str = '/mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/03_input-from-inputs-continuation/checkpoint-3000'\n",
    "    model_checkpoint: str = '/mnt/hdd0/Kaggle/arc24/models/20240910_debug_input_from_inputs/04_input-from-inputs-continuation/checkpoint-8000'\n",
    "    model_checkpoint: str = '/mnt/hdd0/MEGA/projects/temp/20240903_submission_models/08_inputs-RE-ARC-task-augmentation-050-1111_Qwen2-0.5B-Instruct_lr1e-4_r128_4e4steps_10240msl/checkpoint-40000'\n",
    "    merged_model_path: str = '/home/gbarbadillo/data/temp_model'\n",
    "    max_model_len: int = 10240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from vllm import LLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# add path to python path\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "from merge_lora import merge_lora\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from arc24.prompting import create_prompts_from_task, parse_grid_from_response\n",
    "from arc24.encoders import create_grid_encoder\n",
    "from inference import get_sampling_params\n",
    "from evaluation import plot_grids\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_lora(cfg.base_model_path, cfg.model_checkpoint, cfg.merged_model_path)\n",
    "# raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=cfg.merged_model_path,\n",
    "    trust_remote_code=True,\n",
    "    dtype='half',\n",
    "    tensor_parallel_size=2, # to use 2 gpus\n",
    "    max_model_len=cfg.max_model_len,\n",
    "    #kv_cache_dtype='fp8_e5m2', I have disabled kv cache quantization because it is hurtful\n",
    "    enforce_eager=True, # without this 13.9GB of memory is used on each GPU, with this is 13.3GB,\n",
    "    disable_log_stats=True,\n",
    "    max_num_seqs=255, # default is supposed to be 256 I have used it to solve some weird illegal memory error\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.merged_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/new_partitions/train_rs7.json')\n",
    "val_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/new_partitions/val_rs7.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_encoder = create_grid_encoder(\"GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_samples(task_id, tasks, n_generate=10, n_show=5, temperature=0.9):\n",
    "    task = tasks[task_id]\n",
    "    new_task = dict(train=task['train'] + task['test'], test=task['test'])\n",
    "    prompt = create_prompts_from_task(new_task, grid_encoder, tokenizer, is_train_prompt=False, prompt_version='input-from-inputs-v0')[0]\n",
    "    sampling_params = get_sampling_params(best_of=1, temperature=temperature, n=n_generate, max_output_tokens=1224)\n",
    "    outputs = llm.generate(prompt, sampling_params=sampling_params, use_tqdm=True)\n",
    "    grids = []\n",
    "    for output in outputs[0].outputs[:n_show]:\n",
    "        try:\n",
    "            print(output.cumulative_logprob)\n",
    "            grids.append(parse_grid_from_response(output.text, grid_encoder))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(output.text)\n",
    "\n",
    "    plot_grids([sample['input'] for sample in task['train'] + task['test']])\n",
    "    plt.suptitle(f'Original inputs from task {task_id}')\n",
    "    plt.show()\n",
    "\n",
    "    plot_grids(grids)\n",
    "    plt.suptitle(f'Generated samples from task {task_id}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    generate_new_samples(list(train_data.keys())[idx], train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    generate_new_samples(list(val_data.keys())[idx], val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
