{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps with o1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the new OpenAI's o1 model can write python code to solve the challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# add path to python path\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "\n",
    "\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from evaluation import plot_grids, plot_task, plot_grid\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/neoneye/arc-dataset-collection/tree/main/dataset/PQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize tasks from the same gestalt law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tasks_from_same_gestalt_law(folder, n=5):\n",
    "    filepaths = sorted(glob.glob(os.path.join(folder, '*.json')))\n",
    "    filepaths = random.choices(filepaths, k=n)\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, 'r') as f:\n",
    "            task = json.load(f)\n",
    "        plot_task(task)\n",
    "        plt.suptitle(os.path.splitext(os.path.basename(filepath))[0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/data/PQA/PQA-dataset_10k/pqa-dataset/*')):\n",
    "    print(os.path.basename(folder))\n",
    "    visualize_tasks_from_same_gestalt_law(folder, n=5)\n",
    "    print('\\n'*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see any difference between tasks of the same folder. We only have 7 different tasks, we could group all the tasks together in the same style as I do with RE-ARC.\n",
    "\n",
    "All the tasks have 6 train samples and 3 test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group all tasks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_pqa_tasks(folder, max_tasks_per_category=100):\n",
    "    task_folders = sorted(glob.glob(os.path.join(folder, '*')))\n",
    "    tasks = {}\n",
    "    for task_folder in tqdm(task_folders):\n",
    "        task_name = os.path.basename(task_folder)\n",
    "        tasks[task_name] = dict(train=list(), n_train=6)\n",
    "        for filepath in islice(sorted(glob.glob(os.path.join(task_folder, '*.json'))), max_tasks_per_category):\n",
    "            with open(filepath, 'r') as f:\n",
    "                task = json.load(f)\n",
    "            tasks[task_name]['train'].extend(task['train'])\n",
    "            tasks[task_name]['train'].extend(task['test'])\n",
    "        print(f'{task_name} tasks: {len(tasks[task_name][\"train\"])}')\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = group_pqa_tasks('/mnt/hdd0/Kaggle/arc24/data/PQA/PQA-dataset_10k/pqa-dataset', max_tasks_per_category=100)\n",
    "with open('/mnt/hdd0/Kaggle/arc24/data/PQA/PQA-dataset_10k/pqa-dataset-1k.json', 'w') as f:\n",
    "    json.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = group_pqa_tasks('/mnt/hdd0/Kaggle/arc24/data/PQA/PQA-dataset_10k/pqa-dataset', max_tasks_per_category=1000)\n",
    "with open('/mnt/hdd0/Kaggle/arc24/data/PQA/PQA-dataset_10k/pqa-dataset-10k.json', 'w') as f:\n",
    "    json.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks.values():\n",
    "    for sample in tqdm(task['train']):\n",
    "        assert np.min(sample['input']) >= 0\n",
    "        assert np.min(sample['output']) >= 0\n",
    "        assert np.max(sample['input']) <= 9\n",
    "        assert np.max(sample['output']) <= 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probes that there are no weird colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_extra_kaggle_datasets():\n",
    "    tasks = dict()\n",
    "    for filepath in sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/data/kaggle/*/*.json')):\n",
    "        with open(filepath, 'r') as f:\n",
    "            task = json.load(f)\n",
    "        tasks[os.path.splitext(os.path.basename(filepath))[0]] = task\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = load_extra_kaggle_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_id, task in tasks.items():\n",
    "    plot_task(task)\n",
    "    plt.suptitle(task_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc24/data/kaggle/kaggle.json', 'w') as f:\n",
    "    json.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neoeye datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARC dataset tama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/neoneye/arc-dataset-tama/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize tasks from the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tasks_from_same_folder(folder, n=5):\n",
    "    filepaths = sorted(glob.glob(os.path.join(folder, '*.json')))\n",
    "    filepaths = random.choices(filepaths, k=n)\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, 'r') as f:\n",
    "            task = json.load(f)\n",
    "        plot_task(task)\n",
    "        plt.suptitle(os.path.splitext(os.path.basename(filepath))[0])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in sorted(glob.glob('/mnt/hdd0/Kaggle/arc24/data/arc-dataset-tama/dataset/*')):\n",
    "    print(os.path.basename(folder))\n",
    "    visualize_tasks_from_same_folder(folder, n=5)\n",
    "    print('\\n'*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks are repeated, but they use different colors. Thus I could not group them in the same task. Instead I'm thinking of grouping them in a list, and select a random element from the list when training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curate_tama_dataset(folder):\n",
    "    tasks = dict()\n",
    "    subfolders = sorted(glob.glob(os.path.join(folder, '*')))\n",
    "    for subfolder in tqdm(subfolders):\n",
    "        task_name = os.path.basename(subfolder)\n",
    "        tasks[task_name] = list()\n",
    "        for filepath in sorted(glob.glob(os.path.join(subfolder, '*.json'))):\n",
    "            with open(filepath, 'r') as f:\n",
    "                task = json.load(f)\n",
    "            task.pop('metadata')\n",
    "            tasks[task_name].append(task)\n",
    "        print(f'{task_name} tasks: {len(tasks[task_name])}')\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = curate_tama_dataset('/mnt/hdd0/Kaggle/arc24/data/arc-dataset-tama/dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 50 different tasks, each with 100 variations. It's a good dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc24/data/neoeye_tama.json', 'w') as f:\n",
    "    json.dump(tasks, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "\n",
    "class DecodeRLEError(ValueError):\n",
    "    \"\"\"Exception raised for errors in RLE decoding.\"\"\"\n",
    "    def __init__(self, message: str, details: Optional[str] = None):\n",
    "        super().__init__(message)\n",
    "        self.details = details\n",
    "\n",
    "def decode_rle_row_inner(row: str) -> List[int]:\n",
    "    if not row:\n",
    "        raise DecodeRLEError(\"Invalid row: row cannot be empty\")\n",
    "\n",
    "    decoded_row = []\n",
    "    prev_count = 1\n",
    "    x = 0\n",
    "    current_az_count = 0\n",
    "\n",
    "    for ch in row:\n",
    "        if ch.isdigit():\n",
    "            color = int(ch)\n",
    "            for _ in range(prev_count):\n",
    "                decoded_row.append(color)\n",
    "                x += 1\n",
    "            prev_count = 1\n",
    "            current_az_count = 0\n",
    "        else:\n",
    "            if not ('a' <= ch <= 'z'):\n",
    "                raise DecodeRLEError(\"Invalid character inside row\", details=f\"Character: {ch}\")\n",
    "            current_az_count += 1\n",
    "            if current_az_count >= 2:\n",
    "                raise DecodeRLEError(\"No adjacent a-z characters are allowed\", details=f\"Character: {ch}\")\n",
    "            count = ord(ch) - ord('a') + 2\n",
    "            prev_count = count\n",
    "\n",
    "    if current_az_count > 0:\n",
    "        raise DecodeRLEError(\"Last character must not be a-z character\", details=f\"Character: {ch}\")\n",
    "\n",
    "    return decoded_row\n",
    "\n",
    "def decode_rle_row(row: str, width: int) -> List[int]:\n",
    "    if not row:\n",
    "        return []\n",
    "\n",
    "    if len(row) == 1:\n",
    "        ch = row[0]\n",
    "        if ch.isdigit():\n",
    "            color = int(ch)\n",
    "            return [color] * width\n",
    "        else:\n",
    "            raise DecodeRLEError(\"Invalid character for full row\", details=f\"Character: {ch}\")\n",
    "\n",
    "    decoded_row = decode_rle_row_inner(row)\n",
    "    length_decoded_row = len(decoded_row)\n",
    "    if length_decoded_row != width:\n",
    "        raise DecodeRLEError(\"Mismatch between width and the number of RLE columns\",\n",
    "                             details=f\"Expected width: {width}, Decoded width: {length_decoded_row}\")\n",
    "\n",
    "    return decoded_row\n",
    "\n",
    "def deserialize(input_str: str) -> np.array:\n",
    "    verbose = False\n",
    "\n",
    "    parts = input_str.split(' ')\n",
    "    count_parts = len(parts)\n",
    "    if count_parts != 3:\n",
    "        raise DecodeRLEError(\"Expected 3 parts\", details=f\"But got {count_parts} parts\")\n",
    "\n",
    "    width_str, height_str, rows_str = parts\n",
    "    rows = rows_str.split(',')\n",
    "\n",
    "    # Validate width and height strings\n",
    "    try:\n",
    "        width = int(width_str)\n",
    "        height = int(height_str)\n",
    "    except ValueError as e:\n",
    "        raise DecodeRLEError(\n",
    "            \"Cannot parse width and height\",\n",
    "            details=str(e)\n",
    "        )\n",
    "\n",
    "    # Images with negative dimensions cannot be created\n",
    "    if width < 0 or height < 0:\n",
    "        raise DecodeRLEError(\"Width and height must non-negative\")\n",
    "\n",
    "    count_rows = len(rows)\n",
    "    if count_rows != height:\n",
    "        raise DecodeRLEError(\"Mismatch between height and the number of RLE rows\",\n",
    "                             details=f\"Expected height: {height}, Number of rows: {count_rows}\")\n",
    "\n",
    "    image = np.zeros((height, width), dtype=np.uint8)\n",
    "    copy_y = 0\n",
    "\n",
    "    for y in range(height):\n",
    "        row = rows[y]\n",
    "        if verbose:\n",
    "            print(f\"y: {y} row: {row}\")\n",
    "        if not row:\n",
    "            if y == 0:\n",
    "                raise DecodeRLEError(\"First row is empty\")\n",
    "            image[y, :] = image[copy_y, :]\n",
    "            continue\n",
    "        copy_y = y\n",
    "        decoded_row = decode_rle_row(row, width)\n",
    "        image[y, :] = decoded_row\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I0 2 7 90,9,30,09,0,03,0 O0 2 7 0,03,0,09,30,9,90 I1 7 3 07c18,a7c38,7 O1 7 3 7,a7c38,07c18 I2 3 11 6,616,626,,,6a2,6,,,, O2 3 11 6,,,,,6a2,626,,,616,6 I3T 11 9 37a3a7376a3,3a232a73262,7a37362a373,27a2323a6a3,a26b73a723,a2373a2a323,27a36726a32,73273a727a2,7a367b3237 O3T 11 9 37a373a76a3,3a232a73262,73273632373,2732323a6a3,a26b7a3273,a27a3a27b3,23736a763a2,a32737a27a2,a7367b3237\"\"\"\n",
    "decode_rle_row(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"2 7 90,9,30,09,0,03,0 O0 2 7 0,03,0,09,30,9,90\"\"\"\n",
    "deserialize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_summary(filepath):\n",
    "    tasks = load_arc_data_with_solutions(filepath)\n",
    "    print(f\"Number of tasks: {len(tasks)}\")\n",
    "    samples_per_task = []\n",
    "    for task in tasks.values():\n",
    "        if isinstance(task, dict):\n",
    "            samples_per_task.append(len(task['train']) + len(task['test']))\n",
    "        elif isinstance(task, list):\n",
    "            samples_per_task.append(sum([len(subtask['train']) + len(subtask['test']) for subtask in task]))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid task type: {type(task)}\")\n",
    "    print(f\"Mean number of samples per task: {np.mean(samples_per_task):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataset_summary('../data/external_data/MINI-ARC.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataset_summary('../data/external_data/kaggle.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dataset_summary('../data/external_data/neoeye_tama.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
