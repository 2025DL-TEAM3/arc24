{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write python code for training tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write python code that implements training tasks and also creates the input distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import inspect\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# add path to python path\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from evaluation import plot_grids, plot_grid\n",
    "from arc24.logging import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "sys.path.append(os.path.realpath('../arc'))\n",
    "import training_inputs\n",
    "import training_tasks\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 2)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json')\n",
    "train_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grids_with_shape(grids, suptitle=None, facecolor='white'):\n",
    "    plt.figure(facecolor=facecolor)\n",
    "    for plot_idx, grid in enumerate(grids):\n",
    "        plt.subplot(1, len(grids), plot_idx + 1)\n",
    "        plot_grid(grid)\n",
    "        plt.title(f'{len(grid)}x{len(grid[0])}')\n",
    "    if suptitle is not None:\n",
    "        plt.suptitle(suptitle)\n",
    "        plt.tight_layout(pad=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_train_task(task_id):\n",
    "    print(task_id)\n",
    "\n",
    "    inputs = [sample['input'] for sample in train_data[task_id]['train'] + train_data[task_id]['test']]\n",
    "    outputs = [sample['output'] for sample in train_data[task_id]['train'] + train_data[task_id]['test']]\n",
    "    plot_grids_with_shape(inputs, 'Task Inputs')\n",
    "    plot_grids_with_shape(outputs, 'Ground truth Outputs', facecolor='gray')\n",
    "\n",
    "    try:\n",
    "        inputs = [getattr(training_inputs, f'task_{task_id}')() for _ in range(5)]\n",
    "        plot_grids_with_shape(inputs, 'Generated Inputs')\n",
    "    except AttributeError:\n",
    "        logger.warning('Input generation function not found')\n",
    "    except NameError:\n",
    "        logger.warning('Input generation is implemented, but it is calling not implemented functions')\n",
    "    try:\n",
    "        outputs = [getattr(training_tasks, f'task_{task_id}')(i) for i in inputs]\n",
    "        plot_grids_with_shape(outputs, 'Generated Outputs', facecolor='gray')\n",
    "    except AttributeError:\n",
    "        logger.warning('Task function not found')\n",
    "    except NameError:\n",
    "        logger.warning('Task function is implemented, but it is calling not implemented functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Define the colors and the corresponding positions\n",
    "colors = [(1, 0, 0), (1, 1, 0), (0, 1, 0)]  # Red, Yellow, Green\n",
    "positions = [0, 0.5, 1]  # At 0 -> red, 0.5 -> yellow, 1 -> green\n",
    "# Create the colormap\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", list(zip(positions, colors)))\n",
    "\n",
    "def measure_progress(module):\n",
    "    progress = []\n",
    "    task_ids = list(train_data.keys())\n",
    "    for task_id in task_ids:\n",
    "        try:\n",
    "            task_function = getattr(module, f'task_{task_id}')\n",
    "            function_parameters = inspect.signature(task_function).parameters\n",
    "            if function_parameters:\n",
    "                task_function(**create_dummy_parameters(function_parameters))\n",
    "            else:\n",
    "                task_function()\n",
    "            progress.append('done')\n",
    "        except AttributeError as e:\n",
    "            progress.append('not implemented')\n",
    "        except NameError:\n",
    "            progress.append('implemented but not functional')\n",
    "    numeric_progress = map({'done': 1, 'not implemented': 0, 'implemented but not functional': 0.5}.get, progress)\n",
    "    numeric_progress = np.array(list(numeric_progress))\n",
    "    print(f'Fully functional tasks: {np.mean(numeric_progress == 1):.1%} ({np.sum(numeric_progress == 1)})')\n",
    "    print(f'Implemented tasks: {np.mean(numeric_progress > 0):.1%} ({np.sum(numeric_progress > 0)})')\n",
    "    plt.imshow(np.array(list(numeric_progress)).reshape(1, -1), cmap=custom_cmap, aspect='auto')\n",
    "\n",
    "def create_dummy_parameters(function_parameters):\n",
    "    kwargs = {}\n",
    "    if 'grid' in function_parameters:\n",
    "        kwargs['grid'] = np.zeros((3, 3), dtype=int).tolist()\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid([np.arange(10).tolist()], write_numbers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_train_task(task_id=list(train_data.keys())[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_progress(training_tasks); plt.title('Task implementation progress');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_progress(training_inputs); plt.title('Input generation progress');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Create stats about the progress of the tasks implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
