{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new inputs v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this second notebook the generation of inputs will be done using the `inference` script, I will use this notebook\n",
    "to visualize the generated inputs and optimize the data and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "from evaluation import plot_grid\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from voting import get_unique_matrices_and_counts_sorted\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to create a new version of the data that uses all the available samples as test samples.\n",
    "That way I will create wrong predictions for all the samples of each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_test_dataset(dataset):\n",
    "    all_test_dataset = {}\n",
    "    for task_id, task in dataset.items():\n",
    "        samples = task['train'] + task['test']\n",
    "        for idx, sample in enumerate(samples):\n",
    "            all_test_dataset[f'{task_id}_{idx}'] = dict(\n",
    "                train=samples[:idx] + samples[idx+1:],\n",
    "                test=[sample],\n",
    "            )\n",
    "    print(f'Created dataset with {len(all_test_dataset)} samples from {len(dataset)} tasks')\n",
    "    return all_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all_test_dataset = create_all_test_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc24/data/all_test/training.json', 'w') as f:\n",
    "    json.dump(training_all_test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/arc-agi_evaluation_challenges.json')\n",
    "evaluation_all_test_dataset = create_all_test_dataset(evaluation_data)\n",
    "with open('/mnt/hdd0/Kaggle/arc24/data/all_test/evaluation.json', 'w') as f:\n",
    "    json.dump(evaluation_all_test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "export temperature=9e-1\n",
    "python inference.py \\\n",
    "--model_path /mnt/hdd0/Kaggle/arc24/models/20240925_submission_models/06_continue-full-fine-tuning-Qwen2.5-0.5B-Instruct_lr1.5e-5_1e5steps_2gpus_8192msl/checkpoint-100000 \\\n",
    "--prompt_version output-from-examples-v1 \\\n",
    "--dataset_path /mnt/hdd0/Kaggle/arc24/data/all_test/training.json \\\n",
    "--output_filepath /mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps_t${temperature}.json \\\n",
    "--predictions_per_task 8 \\\n",
    "--temperature ${temperature}\n",
    "\n",
    "\n",
    "export temperature=9e-1\n",
    "python inference.py \\\n",
    "--model_path /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000 \\\n",
    "--prompt_version output-from-examples-v1 \\\n",
    "--dataset_path /mnt/hdd0/Kaggle/arc24/data/all_test/evaluation.json \\\n",
    "--output_filepath /mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/evaluation_x8_t${temperature}.json \\\n",
    "--predictions_per_task 8 \\\n",
    "--temperature ${temperature}\n",
    "\n",
    "\n",
    "rsync -avP --prune-empty-dirs /mnt/hdd0/MEGA/projects/temp/ /mnt/hdd0/Kaggle/arc24/models/\n",
    "for checkpoint_folder in /mnt/hdd0/Kaggle/arc24/models/20241022_no_training/*/checkpoint-*; do\n",
    "    for temperature in 0.1 0.5 1 1.5; do\n",
    "    \tpython easy_inference_and_evaluation.py \"${checkpoint_folder}\" --dataset_path /mnt/hdd0/Kaggle/arc24/data/all_test/training.json --predictions_per_task 8 --temperature ${temperature}\n",
    "    done\n",
    "done\n",
    "```\n",
    "\n",
    "1200 seconds to generate 8 predictions per task for the 1718 tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine multiple prediction files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_multiple_prediction_files(output_filepath, *input_filepaths):\n",
    "    predictions = []\n",
    "    for filepath in tqdm(input_filepaths, desc='loading files'):\n",
    "        with open(filepath, 'r') as f:\n",
    "            predictions.append(json.load(f))\n",
    "    combined_predictions = dict()\n",
    "    for task_id in tqdm(predictions[0], desc='grouping predictions'):\n",
    "        outputs = []\n",
    "        for prediction in predictions:\n",
    "            outputs.extend(list(prediction[task_id][0].values()))\n",
    "        outputs, _ = get_unique_matrices_and_counts_sorted(outputs)\n",
    "        combined_predictions[task_id] = [{f'attempt_{idx}': output for idx, output in enumerate(outputs, 1)}]\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(combined_predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filepaths = glob.glob('/mnt/hdd0/Kaggle/arc24/evaluations/20241022_no_training/*/checkpoint-*/inference_all-test-training_*.json')\n",
    "input_filepaths.extend(glob.glob('/mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps*.json'))\n",
    "input_filepaths = [filepath for filepath in input_filepaths if not filepath.endswith('_task_results.json')]\n",
    "combine_multiple_prediction_files(\"/mnt/hdd0/Kaggle/arc24/evaluations/20241022_no_training/unique_predictions.json\", *input_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize generated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generated_outputs(filepath, max_plots, random_seed=None, dataset_filepath='/mnt/hdd0/Kaggle/arc24/data/all_test/training.json'):\n",
    "    with open(dataset_filepath, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    random.seed(random_seed)\n",
    "    task_ids = random.sample(list(data.keys()), min(len(data), max_plots))\n",
    "\n",
    "    for task_id in task_ids:\n",
    "        for sample_generations in data[task_id]:\n",
    "            outputs = list(sample_generations.values())\n",
    "            outputs = [output for output in outputs if output]\n",
    "            outputs, counts = get_unique_matrices_and_counts_sorted(outputs)\n",
    "            if len(outputs) <= 1:\n",
    "                continue\n",
    "            print(task_id)\n",
    "            for plot_idx, (output, count) in enumerate(zip(outputs, counts), 1):\n",
    "                plt.subplot(1, len(sample_generations), plot_idx)\n",
    "                plot_grid(output)\n",
    "                title = f'Count: {count}'\n",
    "                if output == dataset[task_id]['test'][0]['output']:\n",
    "                    title = f'Correct! {title}'\n",
    "                plt.title(title)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_generated_outputs('/mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps_t1.4.json', 10, random_seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20241022_no_training/01_lora064-Qwen2.5-0.5B-Instruct_lr1e-4_bs16_20000steps_2gpus_8192msl/checkpoint-20000/inference_all-test-training_x008_t1e+00.json'\n",
    "visualize_generated_outputs(filepath, 10, random_seed=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure available training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure how many training samples we have generated. A training sample is a unique prediction that is different to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_available_training_samples(inference_filepath,\n",
    "                                       dataset_filepath='/mnt/hdd0/Kaggle/arc24/data/all_test/training.json'):\n",
    "    with open(inference_filepath, 'r') as f:\n",
    "        inference = json.load(f)\n",
    "    with open(dataset_filepath, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    available_training_samples = dict()\n",
    "    for task_id in inference:\n",
    "        predictions = list(inference[task_id][0].values())\n",
    "        predictions = [prediction for prediction in predictions if prediction]\n",
    "        unique_predictions, _ = get_unique_matrices_and_counts_sorted(predictions)\n",
    "        n_training_samples = 0\n",
    "        for prediction in unique_predictions:\n",
    "            if prediction != dataset[task_id]['test'][0]['output']:\n",
    "                n_training_samples += 1\n",
    "        available_training_samples[task_id] = n_training_samples\n",
    "    print(f'Available training samples: {sum(available_training_samples.values())}')\n",
    "    print(f'Mean available training samples per task: {sum(available_training_samples.values()) / len(available_training_samples):.2f}')\n",
    "    return available_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_training_samples = measure_available_training_samples('/mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps_t1.2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/mnt/hdd0/Kaggle/arc24/evaluations/20241022_no_training/01_lora064-Qwen2.5-0.5B-Instruct_lr1e-4_bs16_10000steps_2gpus_8192msl/checkpoint-10000/inference_all-test-training_x008_t2e+00.json'\n",
    "available_training_samples = measure_available_training_samples(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_training_samples = measure_available_training_samples(\n",
    "    '/mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/evaluation_x8_t9e-1.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/data/all_test/evaluation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/mnt/hdd0/Kaggle/arc24/evaluations/20241022_no_training/unique_predictions.json\"\n",
    "available_training_samples = measure_available_training_samples(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(available_training_samples.values(), bins=np.arange(-0.5, max(available_training_samples.values()) + 1))\n",
    "plt.xlabel('Number of available training samples per task')\n",
    "plt.ylabel('Number of tasks')\n",
    "plt.grid()\n",
    "plt.title('Distribution of available training samples per task');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [ ] Measure available training samples per inference file. (predictions that are wrong)\n",
    "- [ ] Do I have a way to concatenate predictions?\n",
    "- [ ] Study temperature influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
