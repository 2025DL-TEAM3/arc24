{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new inputs v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this second notebook the generation of inputs will be done using the `inference` script, I will use this notebook\n",
    "to visualize the generated inputs and optimize the data and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "from evaluation import plot_grid\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from voting import get_unique_matrices_and_counts_sorted\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to create a new version of the data that uses all the available samples as test samples.\n",
    "That way I will create wrong predictions for all the samples of each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/arc-agi_training_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_test_dataset(dataset):\n",
    "    all_test_dataset = {}\n",
    "    for task_id, task in dataset.items():\n",
    "        samples = task['train'] + task['test']\n",
    "        for idx, sample in enumerate(samples):\n",
    "            all_test_dataset[f'{task_id}_{idx}'] = dict(\n",
    "                train=samples[:idx] + samples[idx+1:],\n",
    "                test=[sample],\n",
    "            )\n",
    "    print(f'Created dataset with {len(all_test_dataset)} samples from {len(dataset)} tasks')\n",
    "    return all_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_all_test_dataset = create_all_test_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc24/data/all_test/training.json', 'w') as f:\n",
    "    json.dump(training_all_test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "export temperature=9e-1\n",
    "python inference.py \\\n",
    "--model_path /mnt/hdd0/Kaggle/arc24/models/20240925_submission_models/06_continue-full-fine-tuning-Qwen2.5-0.5B-Instruct_lr1.5e-5_1e5steps_2gpus_8192msl/checkpoint-100000 \\\n",
    "--prompt_version output-from-examples-v1 \\\n",
    "--dataset_path /mnt/hdd0/Kaggle/arc24/data/all_test/training.json \\\n",
    "--output_filepath /mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps_t${temperature}.json \\\n",
    "--predictions_per_task 8 \\\n",
    "--temperature ${temperature}\n",
    "```\n",
    "\n",
    "1200 seconds to generate 8 predictions per task for the 1718 tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize generated outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generated_outputs(filepath, max_plots, random_seed=None):\n",
    "    with open(filepath, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    random.seed(random_seed)\n",
    "    task_ids = random.sample(list(data.keys()), min(len(data), max_plots))\n",
    "\n",
    "    for task_id in task_ids:\n",
    "        for sample_generations in data[task_id]:\n",
    "            outputs = list(sample_generations.values())\n",
    "            outputs = [output for output in outputs if output]\n",
    "            outputs, counts = get_unique_matrices_and_counts_sorted(outputs)\n",
    "            if len(outputs) <= 1:\n",
    "                continue\n",
    "            print(task_id)\n",
    "            for plot_idx, (output, count) in enumerate(zip(outputs, counts), 1):\n",
    "                plt.subplot(1, len(sample_generations), plot_idx)\n",
    "                plot_grid(output)\n",
    "                plt.title(f'Count: {count}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_generated_outputs('/mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps_t1.4.json', 10, random_seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure available training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure how many training samples we have generated. A training sample is a unique prediction that is different to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_available_training_samples(inference_filepath,\n",
    "                                       dataset_filepath='/mnt/hdd0/Kaggle/arc24/data/all_test/training.json'):\n",
    "    with open(inference_filepath, 'r') as f:\n",
    "        inference = json.load(f)\n",
    "    with open(dataset_filepath, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    available_training_samples = dict()\n",
    "    for task_id in inference:\n",
    "        predictions = list(inference[task_id][0].values())\n",
    "        predictions = [prediction for prediction in predictions if prediction]\n",
    "        unique_predictions, _ = get_unique_matrices_and_counts_sorted(predictions)\n",
    "        n_training_samples = 0\n",
    "        for prediction in unique_predictions:\n",
    "            if prediction != dataset[task_id]['test'][0]['output']:\n",
    "                n_training_samples += 1\n",
    "        available_training_samples[task_id] = n_training_samples\n",
    "    print(f'Available training samples: {sum(available_training_samples.values())}')\n",
    "    print(f'Mean available training samples per task: {sum(available_training_samples.values()) / len(available_training_samples):.2f}')\n",
    "    return available_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_training_samples = measure_available_training_samples('/mnt/hdd0/Kaggle/arc24/debug/outputs_for_verifiying/first_steps_t1.4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(available_training_samples.values(), bins=np.arange(-0.5, 32))\n",
    "plt.xlabel('Number of available training samples per task')\n",
    "plt.ylabel('Number of tasks')\n",
    "plt.title('Distribution of available training samples per task');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [ ] Measure available training samples per inference file. (predictions that are wrong)\n",
    "- [ ] Do I have a way to concatenate predictions?\n",
    "- [ ] Study temperature influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
