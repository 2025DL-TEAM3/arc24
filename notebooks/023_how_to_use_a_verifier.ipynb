{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use a verifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discover how can we use a verifier to improve prediction selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate,\n",
    "    plot_grid,\n",
    "    print_metrics,)\n",
    "from voting import (\n",
    "    select_most_voted_solutions,\n",
    "    select_most_voted_solutions_solving_ties_with_logprob,\n",
    "    get_unique_matrices_and_counts_sorted\n",
    ")\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first select which prediction models are we going to use. They should be models that haven't been trained with the test set.\n",
    "\n",
    "These models could be used for the task, 3 different models trained without the evaluation dataset.\n",
    "\n",
    "- /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl\n",
    "- /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl\n",
    "- /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is voting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how good is voting for the models selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_voting_accuracy(filepath, ground_truth_filepath='/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'):\n",
    "    ground_truth = load_arc_data_with_solutions(ground_truth_filepath)\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    print(filepath)\n",
    "    metrics = evaluate(ground_truth, solutions, verbose=False)[0]\n",
    "    metrics['n'] = len(list(solutions.values())[0][0])\n",
    "    metrics['naive_voting'] = dict()\n",
    "    metrics['advanced_voting'] = dict()\n",
    "\n",
    "    for i in range(1, 3):\n",
    "        metrics['naive_voting'][i] = evaluate(ground_truth, select_most_voted_solutions(solutions, i), verbose=False)[0].get('pass_n', 0)\n",
    "\n",
    "\n",
    "    with open(filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "        task_results = json.load(f)\n",
    "    for i in range(1, 3):\n",
    "        metrics['advanced_voting'][i] = evaluate(ground_truth, select_most_voted_solutions_solving_ties_with_logprob(task_results, i), verbose=False)[0].get('pass_n', 0)\n",
    "    # print_metrics(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_voting_accuracy('/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x064.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x032.json',\n",
    "]\n",
    "names = ['LoRA-032', 'LoRA-128', 'full fine-tuning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = dict()\n",
    "for name, filepath in zip(names, filepaths):\n",
    "    all_metrics[name] = evaluate_voting_accuracy(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metrics in all_metrics.items():\n",
    "    print(name, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for plot_idx, top_n in enumerate([1, 2], 1):\n",
    "    plt.subplot(1, 2, plot_idx)\n",
    "    categories, values = [], []\n",
    "    for name, metrics in all_metrics.items():\n",
    "        for key in ['advanced_voting']:\n",
    "            categories.append(f'{name} \\npass_32={metrics[\"pass_n\"]:.1%}')\n",
    "            values.append(metrics[key][top_n]/metrics['pass_n'])\n",
    "    plt.bar(categories, values)\n",
    "    plt.grid()\n",
    "    plt.ylabel('Voting accuracy')\n",
    "    plt.title(f'Top_n={top_n}')\n",
    "    plt.ylim(bottom=0.5)\n",
    "plt.suptitle('Voting accuracy of selecting the correct solution among top_n most voted solutions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these models voting is able to select the best response in the first position around 60% of the times, and around 70% in the top two positions.\n",
    "\n",
    "To see how well the verifiers can select the correct answer I could focus on a single model, optimize everything for that model and then check if it works as well for the other models.\n",
    "I could exclude for the optimization all the tasks that do not have a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for plot_idx, top_n in enumerate([1, 2], 1):\n",
    "    plt.subplot(1, 2, plot_idx)\n",
    "    categories, values = [], []\n",
    "    for name, metrics in all_metrics.items():\n",
    "        for key in ['naive_voting', 'advanced_voting']:\n",
    "            categories.append(f'{name} \\n{key}\\npass_32={metrics[\"pass_n\"]:.1%}')\n",
    "            values.append(metrics[key][top_n])\n",
    "    plt.bar(categories, values)\n",
    "    plt.grid()\n",
    "    plt.ylabel('Absolute accuracy')\n",
    "    plt.title(f'Top_n={top_n}')\n",
    "    plt.ylim(bottom=0.15)\n",
    "plt.suptitle('Absolute accuracy of the system when using voting')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study distribution of the number of unique responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_distribution_of_the_number_of_unique_responses(filepath, name, ground_truth_filepath='/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'):\n",
    "    ground_truth = load_arc_data_with_solutions(ground_truth_filepath)\n",
    "    with open(filepath, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    correct, incorrect = [], []\n",
    "    for task_id, task_predictions in predictions.items():\n",
    "        for sample_idx, sample_predictions in enumerate(task_predictions):\n",
    "            sample_predictions = list(sample_predictions.values())\n",
    "            sample_predictions = [prediction for prediction in sample_predictions if prediction]\n",
    "            unique_predictions, _ = get_unique_matrices_and_counts_sorted(sample_predictions)\n",
    "            if ground_truth[task_id]['test'][sample_idx]['output'] in unique_predictions:\n",
    "                correct.append(len(unique_predictions))\n",
    "            else:\n",
    "                incorrect.append(len(unique_predictions))\n",
    "    plt.title(f'Distribution of the number of unique responses for {name}')\n",
    "    plt.hist(correct, bins=range(0, 32), alpha=0.5, label='correct', density=True)\n",
    "    plt.hist(incorrect, bins=range(0, 32), alpha=0.5, label='incorrect', density=True)\n",
    "    plt.xlabel('Number of unique responses')\n",
    "    plt.legend()\n",
    "    return correct, incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x032.json',\n",
    "]\n",
    "names = ['LoRA-032', 'LoRA-128', 'full fine-tuning']\n",
    "for filepath, name in zip(filepaths, names):\n",
    "    study_distribution_of_the_number_of_unique_responses(filepath, name); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the number of unique responses is different between tasks that have the correct answer and that do not have. On average tasks with a correct response tend to have a smaller number of unique response.\n",
    "\n",
    "But at the same time there are cases with a lot of unique responses that have the correct answer.\n",
    "\n",
    "What we can learn from this data is that the method should be able to a number of responses equal to the number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with verifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
