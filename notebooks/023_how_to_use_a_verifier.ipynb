{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use a verifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discover how can we use a verifier to improve prediction selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to reload changes in python scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(os.path.realpath('../scripts/'))\n",
    "from arc24.data import load_arc_data_with_solutions\n",
    "from evaluation import (\n",
    "    load_arc_data_with_solutions,\n",
    "    evaluate,\n",
    "    plot_grid,\n",
    "    print_metrics,)\n",
    "from voting import (\n",
    "    select_most_voted_solutions,\n",
    "    select_most_voted_solutions_solving_ties_with_logprob,\n",
    "    get_unique_matrices_and_counts_sorted\n",
    ")\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to first select which prediction models are we going to use. They should be models that haven't been trained with the test set.\n",
    "\n",
    "These models could be used for the task, 3 different models trained without the evaluation dataset.\n",
    "\n",
    "- /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl\n",
    "- /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl\n",
    "- /mnt/hdd0/Kaggle/arc24/models/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good is voting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how good is voting for the models selected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_voting_accuracy(filepath, ground_truth_filepath='/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'):\n",
    "    with open(filepath, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "\n",
    "    ground_truth = load_arc_data_with_solutions(ground_truth_filepath)\n",
    "\n",
    "\n",
    "    print(filepath)\n",
    "    metrics = evaluate(ground_truth, predictions, verbose=False)[0]\n",
    "    metrics['n'] = len(list(predictions.values())[0][0])\n",
    "    metrics['naive_voting'] = dict()\n",
    "    metrics['advanced_voting'] = dict()\n",
    "\n",
    "    for i in range(1, 3):\n",
    "        metrics['naive_voting'][i] = evaluate(ground_truth, select_most_voted_solutions(predictions, i), verbose=False)[0].get('pass_n', 0)\n",
    "\n",
    "\n",
    "    with open(filepath.replace('.json', '_task_results.json'), 'r') as f:\n",
    "        task_results = json.load(f)\n",
    "    for i in range(1, 3):\n",
    "        metrics['advanced_voting'][i] = evaluate(ground_truth, select_most_voted_solutions_solving_ties_with_logprob(task_results, i), verbose=False)[0].get('pass_n', 0)\n",
    "    # print_metrics(metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_voting_accuracy('/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x064.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x032.json',\n",
    "]\n",
    "names = ['LoRA-032', 'LoRA-128', 'full fine-tuning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = dict()\n",
    "for name, predictions in zip(names, filepaths):\n",
    "    all_metrics[name] = evaluate_voting_accuracy(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, metrics in all_metrics.items():\n",
    "    print(name, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for plot_idx, top_n in enumerate([1, 2], 1):\n",
    "    plt.subplot(1, 2, plot_idx)\n",
    "    categories, values = [], []\n",
    "    for name, metrics in all_metrics.items():\n",
    "        for key in ['advanced_voting']:\n",
    "            categories.append(f'{name} \\npass_32={metrics[\"pass_n\"]:.1%}')\n",
    "            values.append(metrics[key][top_n]/metrics['pass_n'])\n",
    "    plt.bar(categories, values)\n",
    "    plt.grid()\n",
    "    plt.ylabel('Voting accuracy')\n",
    "    plt.title(f'Top_n={top_n}')\n",
    "    plt.ylim(bottom=0.5)\n",
    "plt.suptitle('Voting accuracy of selecting the correct solution among top_n most voted solutions')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these models voting is able to select the best response in the first position around 60% of the times, and around 70% in the top two positions.\n",
    "\n",
    "To see how well the verifiers can select the correct answer I could focus on a single model, optimize everything for that model and then check if it works as well for the other models.\n",
    "I could exclude for the optimization all the tasks that do not have a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "for plot_idx, top_n in enumerate([1, 2], 1):\n",
    "    plt.subplot(1, 2, plot_idx)\n",
    "    categories, values = [], []\n",
    "    for name, metrics in all_metrics.items():\n",
    "        for key in ['naive_voting', 'advanced_voting']:\n",
    "            categories.append(f'{name} \\n{key}\\npass_32={metrics[\"pass_n\"]:.1%}')\n",
    "            values.append(metrics[key][top_n])\n",
    "    plt.bar(categories, values)\n",
    "    plt.grid()\n",
    "    plt.ylabel('Absolute accuracy')\n",
    "    plt.title(f'Top_n={top_n}')\n",
    "    plt.ylim(bottom=0.15)\n",
    "plt.suptitle('Absolute accuracy of the system when using voting')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study distribution of the number of unique responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def study_distribution_of_the_number_of_unique_responses(predictions, name, ground_truth_filepath='/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'):\n",
    "    if isinstance(predictions, str):\n",
    "        with open(predictions, 'r') as f:\n",
    "            predictions = json.load(f)\n",
    "\n",
    "    ground_truth = load_arc_data_with_solutions(ground_truth_filepath)\n",
    "    correct, incorrect = [], []\n",
    "    for task_id, task_predictions in predictions.items():\n",
    "        for sample_idx, sample_predictions in enumerate(task_predictions):\n",
    "            sample_predictions = list(sample_predictions.values())\n",
    "            sample_predictions = [prediction for prediction in sample_predictions if prediction]\n",
    "            unique_predictions, _ = get_unique_matrices_and_counts_sorted(sample_predictions)\n",
    "            if ground_truth[task_id]['test'][sample_idx]['output'] in unique_predictions:\n",
    "                correct.append(len(unique_predictions))\n",
    "            else:\n",
    "                incorrect.append(len(unique_predictions))\n",
    "    plt.title(f'Distribution of the number of unique responses for {name}')\n",
    "    plt.hist(correct, bins=range(0, 32), alpha=0.5, label='correct', density=True)\n",
    "    plt.hist(incorrect, bins=range(0, 32), alpha=0.5, label='incorrect', density=True)\n",
    "    plt.xlabel('Number of unique responses')\n",
    "    plt.legend()\n",
    "    return correct, incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x032.json',\n",
    "]\n",
    "names = ['LoRA-032', 'LoRA-128', 'full fine-tuning']\n",
    "for predictions, name in zip(filepaths, names):\n",
    "    study_distribution_of_the_number_of_unique_responses(predictions, name); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the number of unique responses is different between tasks that have the correct answer and that do not have. On average tasks with a correct response tend to have a smaller number of unique response.\n",
    "\n",
    "But at the same time there are cases with a lot of unique responses that have the correct answer.\n",
    "\n",
    "What we can learn from this data is that the method should be able to a number of responses equal to the number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First steps with verifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only tasks with at least a correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_tasks_with_at_least_a_correct_answer(filepath, ground_truth_filepath='/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'):\n",
    "    ground_truth = load_arc_data_with_solutions(ground_truth_filepath)\n",
    "    with open(filepath, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    keep_task_ids = []\n",
    "    for task_id, task_predictions in predictions.items():\n",
    "        keep_task = True\n",
    "        for sample_idx, sample_predictions in enumerate(task_predictions):\n",
    "            sample_predictions = list(sample_predictions.values())\n",
    "            if not ground_truth[task_id]['test'][sample_idx]['output'] in sample_predictions:\n",
    "                keep_task = False\n",
    "                break\n",
    "        if keep_task: keep_task_ids.append(task_id)\n",
    "    predictions = {task_id: predictions[task_id] for task_id in keep_task_ids}\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_voting_accuracy(predictions, ground_truth_filepath='/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json'):\n",
    "    ground_truth = load_arc_data_with_solutions(ground_truth_filepath)\n",
    "    metrics = evaluate(ground_truth, predictions, verbose=False)[0]\n",
    "    metrics['n'] = len(list(predictions.values())[0][0])\n",
    "    metrics['naive_voting'] = dict()\n",
    "    for i in range(1, 3):\n",
    "        metrics['naive_voting'][i] = evaluate(ground_truth, select_most_voted_solutions(predictions, i), verbose=False)[0].get('pass_n', 0)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = [\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-128-Qwen2-0.5B-Instruct_lr5e-5_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json',\n",
    "    '/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/01_full-fine-tuning-Qwen2-0.5B-Instruct_lr5e-5_8e4steps_2gpus_8192msl/checkpoint-80000/inference_evaluation_x032.json',\n",
    "]\n",
    "predictions = keep_only_tasks_with_at_least_a_correct_answer(filepaths[0])\n",
    "print(len(predictions))\n",
    "evaluate_voting_accuracy(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/hdd0/Kaggle/arc24/debug/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl_checkpoint-40000_inference_evaluation_x032_just_correct_tasks.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works correctly, when leaving only tasks with at least one correct answer the `pass_n` metric raises very close to 100% as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leave_only_unique_predictions(predictions):\n",
    "    unique_predictions = dict()\n",
    "    for task_id, task_predictions in predictions.items():\n",
    "        unique_predictions[task_id] = []\n",
    "        for sample_predictions in task_predictions:\n",
    "            sample_predictions = list(sample_predictions.values())\n",
    "            sample_predictions = [prediction for prediction in sample_predictions if prediction]\n",
    "            sample_predictions, _ = get_unique_matrices_and_counts_sorted(sample_predictions)\n",
    "            unique_predictions[task_id].append(sample_predictions)\n",
    "    return unique_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_predictions = leave_only_unique_predictions(predictions)\n",
    "len(unique_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge LoRA with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python merge_lora.py  --base_model_path /home/gbarbadillo/data/Qwen2-0.5B-Instruct --lora_path /mnt/hdd0/Kaggle/arc24/models/20241023_first_verifiers/05_verify-and-select_lora032-Qwen2-0.5B-Instruct_lr5e-5_bs32_8000steps_2gpus_8192msl/checkpoint-8000 --output_path /home/gbarbadillo/data/Qwen2-0.5B-Instruct-verifier\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a verifier model to rank the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from arc24.encoders import create_grid_encoder\n",
    "\n",
    "model_path = '/home/gbarbadillo/data/Qwen2-0.5B-Instruct-verifier'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "grid_encoder = create_grid_encoder('GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))')\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    trust_remote_code=True,\n",
    "    dtype='half',\n",
    "    tensor_parallel_size=2, # to use 2 gpus\n",
    "    max_model_len=10240,\n",
    "    #kv_cache_dtype='fp8_e5m2', I have disabled kv cache quantization because it is hurtful\n",
    "    enforce_eager=True, # without this 13.9GB of memory is used on each GPU, with this is 13.3GB,\n",
    "    disable_log_stats=True,\n",
    "    max_num_seqs=255, # default is supposed to be 256 I have used it to solve some weird illegal memory error\n",
    "    swap_space=4, # CPU swap space size (GiB) per GPU, has great influence on RAM but I haven't noticed any performance difference\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import get_sampling_params, generate_outputs_with_batches\n",
    "\n",
    "sampling_params = get_sampling_params(best_of=1, temperature=0, n=1, max_output_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = load_arc_data_with_solutions('/mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc24.data_augmentation import apply_data_augmentation, get_random_color_map, get_random_geometric_augmentation_params\n",
    "from arc24.prompting import create_prompts_from_task\n",
    "from itertools import product\n",
    "\n",
    "def create_prompts(predictions, ground_truth, grid_encoder, tokenizer, prompt_version, verifications_per_prediction):\n",
    "    # TODO: I don't have to revert data augmentation, so I don't care about saving it\n",
    "    # TODO: merge ground truth and predictions to create prompts\n",
    "    prompts = []\n",
    "    for task_id, task_predictions in tqdm(predictions.items(), total=len(predictions), desc='Creating prompts'):\n",
    "        for sample_idx, sample_predictions in enumerate(task_predictions):\n",
    "            for prediction_idx, prediction in enumerate(sample_predictions):\n",
    "                for _ in range(verifications_per_prediction):\n",
    "                    task = ground_truth[task_id].copy()\n",
    "                    task['test'] = [dict(input=task['test'][sample_idx]['input'], output=prediction)]\n",
    "                    data_augmentation_kwargs = get_random_geometric_augmentation_params()\n",
    "                    data_augmentation_kwargs['color_map'] = get_random_color_map(change_background_probability=0.1)\n",
    "                    augmented_task = apply_data_augmentation(task, **data_augmentation_kwargs)\n",
    "                    augmented_task['test_output'] = augmented_task['test'][0]['output']\n",
    "                    prompt = create_prompts_from_task(\n",
    "                        augmented_task, grid_encoder=grid_encoder, tokenizer=tokenizer,\n",
    "                        is_train_prompt=False, prompt_version=prompt_version)[0]\n",
    "                    prompts.append(dict(task_id=task_id,\n",
    "                                        data_augmentation_kwargs=data_augmentation_kwargs,\n",
    "                                        prompt=prompt,\n",
    "                                        sample_idx=sample_idx,\n",
    "                                        prediction_idx=prediction_idx))\n",
    "    return prompts\n",
    "\n",
    "prompts = create_prompts(unique_predictions, ground_truth, grid_encoder, tokenizer,\n",
    "                         prompt_version='verify-output-from-examples-v0',\n",
    "                         verifications_per_prediction=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import generate_outputs_with_batches\n",
    "outputs = generate_outputs_with_batches(llm, prompts, sampling_params, batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying outputs takes on average 27s/it. By comparison the inference with a model generating output grids takes around 39s/it (for model `20241022_no_training/01_lora064-Qwen2.5-0.5B-Instruct_lr1e-4_bs16_20000steps_2gpus_8192msl/checkpoint-20000/inference_all-test-training_x040_t1e+00.json`)\n",
    "\n",
    "So it is clearly faster, 30% faster, but I would expected much faster because we are generating very few output tokens compared to generating a grid. It seems that the long input prompt is dominating over the inference.\n",
    "\n",
    "It has taken just 10 minutes to make 8 verifications per prediction for 120 tasks. In Kaggle it would be slower, but I believe it is fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique([output.outputs[0].text for output in outputs], return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the outputs have only two values: yes or no. We have to collect the predictions and aggregate the results per prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_verification_predictions(outputs, prompts, unique_predictions):\n",
    "    verifications = [output.outputs[0].text == 'yes' for output in outputs]\n",
    "    print(np.unique(verifications, return_counts=True))\n",
    "    aggregated_verifications = {task_id: [np.zeros(len(sample_predictions)) for sample_predictions in task_predictions] for task_id, task_predictions in unique_predictions.items()}\n",
    "    for verification, prompt in zip(verifications, prompts):\n",
    "        if verification:\n",
    "            aggregated_verifications[prompt['task_id']][prompt['sample_idx']][prompt['prediction_idx']] += 1\n",
    "    return aggregated_verifications\n",
    "\n",
    "aggregated_verifications = aggregate_verification_predictions(outputs, prompts, unique_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of verifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_verifications_values = []\n",
    "for task_id, task_verifications in aggregated_verifications.items():\n",
    "    for sample_verifications in task_verifications:\n",
    "        aggregated_verifications_values.extend(sample_verifications)\n",
    "plt.hist(aggregated_verifications_values, bins=np.arange(0.5, max(aggregated_verifications_values)+1), alpha=0.5, density=True)\n",
    "plt.xlabel('Number of verifications')\n",
    "plt.title('Distribution of the number of verifications for prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution looks good! Let's jump to evaluation and see how good this first approach is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_predictions_with_verifications(unique_predictions, aggregated_verifications, n):\n",
    "    selected_predictions = dict()\n",
    "    for task_id, task_predictions in unique_predictions.items():\n",
    "        selected_predictions[task_id] = []\n",
    "        for sample_predictions, sample_verifications in zip(task_predictions, aggregated_verifications[task_id]):\n",
    "            ranking = np.argsort(sample_verifications)[::-1][:n]\n",
    "            selected_predictions[task_id].append({f'attempt_{attempt_idx}': sample_predictions[idx] for attempt_idx, idx in enumerate(ranking, 1)})\n",
    "    return selected_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_top in [1, 2]:\n",
    "    selected_predictions = select_predictions_with_verifications(unique_predictions, aggregated_verifications, n_top)\n",
    "    print(f'Accuracy for n_top {n_top}: {evaluate(ground_truth, selected_predictions, verbose=False)[0][\"pass_n\"]:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Baseline accuracy for voting is 60.8% and 70.8%: `'naive_voting': {1: 0.6083333333333333, 2: 0.7083333333333334}}`\n",
    "- With 4 predictions we get 46.7% and 65.8%\n",
    "- With just 8 predictions we get 54.2% and 71.7%, that is very promising. (10m25 of inference)\n",
    "- With 16 predictions we get 58.3% and 75.8% (20m56 of inference)\n",
    "- With 32 predictions we get 66.2% to 80% (42m)\n",
    "- With 64 predictions we get 66.2% and 77.5%, so it seems we have reached the plateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "python verify_predictions.py \\\n",
    "--model-path /home/gbarbadillo/data/Qwen2-0.5B-Instruct-verifier \\\n",
    "--output-path /mnt/hdd0/Kaggle/arc24/debug/verifier_selected_predictions.json \\\n",
    "--dataset-path /mnt/hdd0/Kaggle/arc24/data/new_partitions/arc-agi_all_challenges.json \\\n",
    "--predictions-path  /mnt/hdd0/Kaggle/arc24/debug/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl_checkpoint-40000_inference_evaluation_x032_just_correct_tasks.json \\\n",
    "--verifications-per-prediction 8\n",
    "python evaluation.py /mnt/hdd0/Kaggle/arc24/debug/verifier_selected_predictions.json\n",
    "\n",
    "/mnt/hdd0/Kaggle/arc24/evaluations/20240921_optimal_train_duration/05_LoRA-032-Qwen2-0.5B-Instruct_lr1e-4_4e4steps_2gpus_8192msl/checkpoint-40000/inference_evaluation_x032.json\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- [] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
