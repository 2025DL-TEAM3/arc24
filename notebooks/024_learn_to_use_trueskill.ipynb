{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to use trueskill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://trueskill.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's execute the sample code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trueskill import Rating, quality_1vs1, rate_1vs1\n",
    "alice, bob = Rating(25), Rating(30)  # assign Alice and Bob's ratings\n",
    "if quality_1vs1(alice, bob) < 0.50:\n",
    "    print('This match seems to be not so fair')\n",
    "alice, bob = rate_1vs1(alice, bob)  # update the ratings after the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alice, bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_1vs1(alice, bob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(quality_1vs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_1vs1(Rating(25), Rating(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_1vs1(Rating(40), Rating(25)), quality_1vs1(Rating(25), Rating(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to be a symmetrical function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_1vs1(Rating(100), Rating(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then we can guess match quality which is equivalent with draw probability of this match using quality_1vs1():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice, bob = Rating(25), Rating(25)  # assign Alice and Bob's ratings\n",
    "print(quality_1vs1(alice, bob))\n",
    "alice, bob = rate_1vs1(alice, bob)  # update the ratings after the match\n",
    "print(quality_1vs1(alice, bob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice.mu - alice.sigma*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate a series of matches where alice will win with probability 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "plt.plot()\n",
    "plt.close('all')\n",
    "plt.rcParams[\"figure.figsize\"] = (25, 4)\n",
    "mpl.rcParams['lines.linewidth'] = 3\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def simulate_leaderboard(n_matches, win_probability):\n",
    "    alice, bob = Rating(), Rating()\n",
    "    alice_score, bob_score = [], []\n",
    "    alice_wins = []\n",
    "    for _ in range(n_matches):\n",
    "        if random.random() < win_probability:\n",
    "            alice, bob = rate_1vs1(alice, bob)\n",
    "            alice_wins.append(1)\n",
    "        else:\n",
    "            bob, alice = rate_1vs1(bob, alice)\n",
    "            alice_wins.append(0)\n",
    "        alice_score.append(alice.mu - alice.sigma*2)\n",
    "        bob_score.append(bob.mu - bob.sigma*2)\n",
    "    plt.plot(alice_score, label='alice')\n",
    "    plt.plot(bob_score, label='bob')\n",
    "    plt.legend(loc=0)\n",
    "    print(alice, bob)\n",
    "    print(np.mean(alice_wins), np.std(alice_wins)/np.sqrt(len(alice_wins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_leaderboard(n_matches=50, win_probability=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
